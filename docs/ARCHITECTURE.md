# 系统架构说明

## 整体流程

### 使用阿里云（Agent 模式）

```
用户提问
    ↓
网页/命令行
    ↓
AgentManager.run()
    ↓
create_agent() ← 创建 Agent（带工具）
    ↓
Agent 决策 ← 调用阿里云千问 LLM
    ↓
    ├─→ 决定使用工具 search_knowledge_base
    │       ↓
    │   检索向量数据库（本地）
    │       ↓
    │   返回相关文档片段
    │       ↓
    │   Agent 再次调用千问 LLM（带上文档内容）
    │       ↓
    └───→ 生成最终答案
            ↓
        返回给用户
```

### 使用 Groq（简化 RAG 模式）

```
用户提问
    ↓
网页/命令行
    ↓
AgentManager.run()
    ↓
run_simple_rag() ← 不使用 Agent
    ↓
直接检索向量数据库（本地）
    ↓
获取相关文档片段
    ↓
构建提示词（问题 + 文档内容）
    ↓
调用 Groq LLM（一次性）
    ↓
生成最终答案
    ↓
返回给用户
```

## 详细组件说明

### 1. 用户界面层

- **网页界面**：`app/static/index.html`
- **命令行界面**：`scripts/chat.py`
- **API 接口**：`app/main.py`

### 2. Agent 层（仅阿里云）

**AgentManager** (`app/core/agent.py`)
- 管理 LLM 和 RAG
- 创建 Agent（阿里云模式）
- 或直接调用 RAG（Groq 模式）

**Agent 的工作流程**：
```python
# 1. Agent 收到问题
"贾宝玉是谁？"

# 2. Agent 调用千问 LLM 决策
千问: "我需要使用 search_knowledge_base 工具"

# 3. Agent 执行工具
search_knowledge_base("贾宝玉是谁？")
    ↓
检索向量数据库
    ↓
返回: "贾宝玉，别号怡红公子..."

# 4. Agent 再次调用千问 LLM
千问: 收到工具返回的内容，生成答案
    ↓
"贾宝玉是《红楼梦》的主角..."

# 5. 返回答案
```

### 3. RAG 层

**RAGManager** (`app/core/rag.py`)
- 管理向量数据库
- 提供检索功能
- 使用阿里云 Embedding 模型

**向量检索流程**：
```python
# 1. 用户问题
"贾宝玉是谁？"

# 2. 问题向量化（Embedding）
阿里云 text-embedding-v3
    ↓
[0.123, 0.456, 0.789, ...] (1536 维向量)

# 3. 在向量数据库中搜索
ChromaDB 计算相似度
    ↓
找到最相似的 3 个文档片段

# 4. 返回文档内容
[
    "贾宝玉，别号怡红公子...",
    "宝玉生于荣国府...",
    "宝玉与黛玉..."
]
```

### 4. LLM 层

**ChatOpenAI** (LangChain 封装)
- 阿里云：`https://dashscope.aliyuncs.com/compatible-mode/v1`
- Groq：`https://api.groq.com/openai/v1`

**LLM 不等于 AI**：
- **LLM**：Large Language Model（大语言模型）
- **千问**：阿里云的 LLM 产品名称
- **Groq**：提供 LLM 推理服务的公司

### 5. 向量数据库层

**ChromaDB** (`vector_store/`)
- 存储文档的向量表示
- 提供相似度搜索
- 本地存储，不需要网络

## 关键概念澄清

### Agent vs LLM

```
Agent（智能体）
    ├─ 包含 LLM（大脑）
    ├─ 包含 Tools（工具）
    └─ 决策循环
        1. 思考：调用 LLM 决定下一步
        2. 行动：执行工具（如搜索知识库）
        3. 观察：获取工具返回结果
        4. 重复：直到得出最终答案

LLM（大语言模型）
    └─ 只是文本生成模型
        输入：文本
        输出：文本
        不能主动调用工具
```

### 为什么 Groq 不用 Agent？

```
阿里云千问：
    ✅ 支持 Function Calling（工具调用）
    ✅ 可以作为 Agent 的大脑
    ✅ 能理解工具定义并决定何时调用

Groq：
    ❌ Function Calling 支持不完善
    ❌ 作为 Agent 会报错
    ✅ 但可以直接生成文本（很快）
    
所以：
    Groq → 简化 RAG（直接把文档内容放进提示词）
    阿里云 → Agent RAG（让 LLM 决定是否搜索）
```

## 完整调用链

### 阿里云模式（2-3 次 API 调用）

```
1. 用户提问："贾宝玉是谁？"
   ↓
2. Agent 调用千问 LLM（第 1 次）
   请求：{
       "messages": [{"role": "user", "content": "贾宝玉是谁？"}],
       "tools": [{"name": "search_knowledge_base", ...}]
   }
   响应：{
       "tool_calls": [{"name": "search_knowledge_base", "arguments": {"query": "贾宝玉"}}]
   }
   ↓
3. 执行工具：search_knowledge_base("贾宝玉")
   ↓
4. 向量检索（本地，不调用 API）
   问题 → Embedding（调用阿里云 API）→ 向量
   向量 → ChromaDB 搜索 → 相关文档
   ↓
5. Agent 调用千问 LLM（第 2 次）
   请求：{
       "messages": [
           {"role": "user", "content": "贾宝玉是谁？"},
           {"role": "tool", "content": "贾宝玉，别号怡红公子..."}
       ]
   }
   响应：{
       "content": "贾宝玉是《红楼梦》的主角..."
   }
   ↓
6. 返回答案给用户
```

**API 调用次数**：
- Embedding API：1 次（向量化问题）
- LLM API：2 次（决策 + 生成答案）

### Groq 模式（1 次 LLM 调用）

```
1. 用户提问："贾宝玉是谁？"
   ↓
2. 向量检索（本地）
   问题 → Embedding（调用阿里云 API）→ 向量
   向量 → ChromaDB 搜索 → 相关文档
   ↓
3. 构建提示词
   prompt = f"""
   知识库内容：
   {文档内容}
   
   用户问题：贾宝玉是谁？
   
   请基于知识库内容回答。
   """
   ↓
4. 调用 Groq LLM（1 次）
   请求：{
       "messages": [{"role": "user", "content": prompt}]
   }
   响应：{
       "content": "贾宝玉是《红楼梦》的主角..."
   }
   ↓
5. 返回答案给用户
```

**API 调用次数**：
- Embedding API：1 次（阿里云，向量化问题）
- LLM API：1 次（Groq，生成答案）

## 费用对比

### 单次提问费用

**阿里云模式**：
- Embedding：¥0.00001（text-embedding-v3）
- LLM 决策：¥0.005（qwen-plus，约 500 tokens）
- LLM 生成：¥0.01（qwen-plus，约 1000 tokens）
- **总计：约 ¥0.015**

**Groq 模式**：
- Embedding：¥0.00001（阿里云）
- LLM 生成：¥0（Groq 免费）
- **总计：约 ¥0.00001**

### 100 次提问费用

| 模式 | Embedding | LLM | 总计 |
|------|-----------|-----|------|
| 阿里云 | ¥0.001 | ¥1.50 | **¥1.50** |
| Groq | ¥0.001 | ¥0 | **¥0.001** |

## 性能对比

### 响应时间

| 模式 | Embedding | LLM 调用 | 总时间 |
|------|-----------|----------|--------|
| 阿里云 | 0.2s | 2s × 2 = 4s | **4.2s** |
| Groq | 0.2s | 0.5s × 1 = 0.5s | **0.7s** |

**Groq 快 6 倍！**

## 总结

### 你的理解修正

❌ **错误理解**：
```
提问 → Agent → LLM → AI(千问) → LLM → Agent
```

✅ **正确理解（阿里云）**：
```
提问 → Agent → 千问 LLM（决策）→ 工具（检索）→ 千问 LLM（生成）→ 答案
```

✅ **正确理解（Groq）**：
```
提问 → 检索 → Groq LLM（生成）→ 答案
```

### 关键点

1. **LLM 就是 AI 模型**（千问、Groq 都是 LLM）
2. **Agent 是框架**（包含 LLM + 工具 + 决策循环）
3. **阿里云用 Agent**（因为支持工具调用）
4. **Groq 不用 Agent**（简化为直接调用）
5. **Embedding 始终用阿里云**（Groq 不提供）

### 架构图

```
┌─────────────────────────────────────────┐
│           用户界面层                     │
│  ┌──────┐  ┌──────┐  ┌──────┐          │
│  │ 网页 │  │ 命令行│  │ API  │          │
│  └──────┘  └──────┘  └──────┘          │
└─────────────────────────────────────────┘
                  ↓
┌─────────────────────────────────────────┐
│         AgentManager（业务层）           │
│  ┌──────────────┐  ┌──────────────┐    │
│  │ Agent 模式   │  │ 简化 RAG 模式 │    │
│  │ (阿里云)     │  │ (Groq)       │    │
│  └──────────────┘  └──────────────┘    │
└─────────────────────────────────────────┘
         ↓                    ↓
┌─────────────────┐  ┌─────────────────┐
│   RAGManager    │  │   LLM 服务      │
│  (向量检索)      │  │                 │
│  ┌───────────┐  │  │  ┌──────────┐  │
│  │ ChromaDB  │  │  │  │ 阿里云千问│  │
│  └───────────┘  │  │  └──────────┘  │
│  ┌───────────┐  │  │  ┌──────────┐  │
│  │ Embedding │  │  │  │   Groq   │  │
│  │ (阿里云)  │  │  │  └──────────┘  │
│  └───────────┘  │  │                 │
└─────────────────┘  └─────────────────┘
```

希望这样解释清楚了！
