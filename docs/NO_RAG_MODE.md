# 💰 纯 LLM 模式（无知识库）

## 什么是纯 LLM 模式？

不使用知识库，直接与 LLM 对话，适合：
- 🧪 调试 LLM
- 💬 通用对话
- 💰 完全免费（使用 Groq）
- 🚀 快速测试

## 使用方法

### 方法 1：使用专用脚本（推荐）

```bash
# 交互式对话
python scripts/chat_llm.py

# 单次提问
python scripts/chat_llm.py "你好，介绍一下自己"
```

### 方法 2：使用 --no-rag 参数

```bash
# 交互式对话
python scripts/chat.py --no-rag

# 单次提问
python scripts/chat.py --no-rag "你好"
```

## 对比

### 普通模式（使用知识库）

```bash
python scripts/chat.py "贾宝玉是谁"
```

**流程：**
```
问题 → Embedding → 检索知识库 → LLM → 回答
```

**费用：**
- Embedding：¥0.00001
- LLM：¥0.01-0.05（阿里云）或 免费（Groq）

### 纯 LLM 模式（无知识库）

```bash
python scripts/chat_llm.py "你好"
```

**流程：**
```
问题 → LLM → 回答
```

**费用：**
- Embedding：¥0（不使用）
- LLM：¥0.01-0.05（阿里云）或 **免费**（Groq）

## 功能特点

### chat_llm.py 特点

1. **对话历史**
   - 自动保存对话历史
   - 支持上下文连续对话
   - 最多保留 10 轮对话

2. **特殊命令**
   - `exit` / `quit` - 退出
   - `clear` - 清空对话历史
   - `history` - 查看对话历史

3. **完全免费**（使用 Groq）
   - 不使用 Embedding
   - 不使用知识库
   - 只调用 Groq LLM

## 使用场景

### 场景 1：调试 LLM

```bash
# 测试不同的提示词
python scripts/chat_llm.py "用简洁的语言解释量子计算"
python scripts/chat_llm.py "用诗歌的形式解释量子计算"
```

### 场景 2：通用对话

```bash
python scripts/chat_llm.py
💬 你的问题：帮我写一首诗
💬 你的问题：翻译成英文
💬 你的问题：再改成七言绝句
```

### 场景 3：快速测试

```bash
# 测试 Groq 连接
python scripts/chat_llm.py "你好"

# 测试响应速度
time python scripts/chat_llm.py "1+1=?"
```

### 场景 4：创意写作

```bash
python scripts/chat_llm.py
💬 你的问题：写一个科幻故事的开头
💬 你的问题：继续写下去
💬 你的问题：加入一个反转
```

## 示例对话

### 示例 1：连续对话

```bash
$ python scripts/chat_llm.py

✅ 使用 Groq 模型: llama-3.3-70b-versatile
💰 完全免费模式（不使用知识库）

======================================================================
🤖 纯 LLM 对话（无知识库）
======================================================================

💡 提示：
  - 直接输入问题，按回车获得答案
  - 输入 'exit' 或 'quit' 退出
  - 输入 'clear' 清屏
  - 不使用知识库，完全免费（Groq）

======================================================================

💬 你的问题：你好

⏳ 正在思考...

🤖 回答：
----------------------------------------------------------------------
你好！我是一个 AI 助手，很高兴与你交流。有什么我可以帮助你的吗？
----------------------------------------------------------------------

📊 数据来源：❌ 纯 LLM（未使用知识库）

======================================================================

💬 你的问题：帮我写一首关于春天的诗

⏳ 正在思考...

🤖 回答：
----------------------------------------------------------------------
春风拂面暖如酥，
万物复苏绿满途。
桃李争妍蝶舞忙，
莺歌燕语报春图。
----------------------------------------------------------------------

📊 数据来源：❌ 纯 LLM（未使用知识库）

======================================================================

💬 你的问题：改成现代诗

⏳ 正在思考...

🤖 回答：
----------------------------------------------------------------------
春天来了
带着温柔的风
唤醒沉睡的大地

花儿绽放
蝴蝶翩翩起舞
鸟儿唱着欢快的歌

这是生命的季节
这是希望的开始
----------------------------------------------------------------------

📊 数据来源：❌ 纯 LLM（未使用知识库）

======================================================================

💬 你的问题：exit

👋 再见！
```

### 示例 2：单次提问

```bash
$ python scripts/chat_llm.py "用一句话解释人工智能"

✅ 使用 Groq 模型: llama-3.3-70b-versatile
💰 完全免费模式（不使用知识库）

======================================================================
🤖 纯 LLM 对话（无知识库）
======================================================================

💬 问题：用一句话解释人工智能

⏳ 正在思考...

🤖 回答：
----------------------------------------------------------------------
人工智能是让计算机模拟人类智能行为的技术，包括学习、推理、
感知和决策等能力。
----------------------------------------------------------------------

📊 数据来源：❌ 纯 LLM（未使用知识库）

======================================================================
```

## 配置

### 使用 Groq（推荐）

```env
MODEL_PROVIDER=groq
GROQ_API_KEY=your_groq_key
GROQ_LLM_MODEL=llama-3.3-70b-versatile
```

**优势：**
- ⚡ 响应速度快
- 💰 完全免费
- 🚀 无需配置 Embedding

### 使用阿里云

```env
MODEL_PROVIDER=aliyun
DASHSCOPE_API_KEY=your_aliyun_key
LLM_MODEL=qwen-plus
```

**优势：**
- 🇨🇳 中文支持最佳
- 🏢 企业级稳定性

## 性能对比

### 响应速度

| 模式 | 提供商 | 平均响应时间 |
|------|--------|-------------|
| 纯 LLM | Groq | 0.5-1 秒 ⚡⚡ |
| 纯 LLM | 阿里云 | 2-3 秒 |
| RAG | Groq | 1-2 秒 ⚡ |
| RAG | 阿里云 | 3-5 秒 |

**结论：纯 LLM + Groq 最快！**

### 费用对比（100 次对话）

| 模式 | 提供商 | 费用 |
|------|--------|------|
| 纯 LLM | Groq | **免费** ✅ |
| 纯 LLM | 阿里云 | ¥1.00 |
| RAG | Groq | ¥0.001 |
| RAG | 阿里云 | ¥1.00 |

**结论：纯 LLM + Groq 完全免费！**

## 限制

### Groq 限制

- 每天有请求次数限制
- 每分钟有速率限制
- 查看：https://console.groq.com/settings/limits

### 功能限制

- ❌ 无法访问本地知识库
- ❌ 无法回答文档相关问题
- ✅ 适合通用对话
- ✅ 适合创意写作
- ✅ 适合快速测试

## 常见问题

### Q: 纯 LLM 模式和 RAG 模式有什么区别？

A: 
- **纯 LLM**：直接对话，不使用知识库，完全免费
- **RAG**：使用知识库，可以回答文档相关问题，需要 Embedding 费用

### Q: 什么时候用纯 LLM 模式？

A: 
- 调试 LLM
- 通用对话
- 不需要知识库
- 想要完全免费

### Q: 什么时候用 RAG 模式？

A: 
- 需要回答文档相关问题
- 需要准确的信息
- 有特定的知识库

### Q: 可以在纯 LLM 模式下切换到 RAG 吗？

A: 不能，需要重新启动脚本。

### Q: 对话历史会保存吗？

A: 只在当前会话中保存，退出后清空。

## 总结

### 推荐使用场景

**纯 LLM 模式（chat_llm.py）：**
- ✅ 调试 LLM
- ✅ 通用对话
- ✅ 创意写作
- ✅ 快速测试
- ✅ 完全免费（Groq）

**RAG 模式（chat.py）：**
- ✅ 文档问答
- ✅ 知识库查询
- ✅ 需要准确信息
- ✅ 企业应用

### 快速命令

```bash
# 纯 LLM（免费）
python scripts/chat_llm.py

# RAG（使用知识库）
python scripts/chat.py

# 纯 LLM（使用 --no-rag）
python scripts/chat.py --no-rag
```

### 最佳配置

```env
# 使用 Groq（完全免费）
MODEL_PROVIDER=groq
GROQ_API_KEY=your_groq_key
GROQ_LLM_MODEL=llama-3.3-70b-versatile
```

**享受完全免费的 AI 对话！** 🎉
