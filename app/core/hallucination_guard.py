"""
åå¹»è§‰å®ˆå« - é™ä½ LLM å¹»è§‰çš„ç­–ç•¥
"""
from typing import List, Dict, Any, Optional
from langchain_core.documents import Document


class HallucinationGuard:
    """åå¹»è§‰å®ˆå«"""
    
    def __init__(self, min_similarity=0.5, min_docs=2):
        """
        åˆå§‹åŒ–åå¹»è§‰å®ˆå«
        
        Args:
            min_similarity: æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
            min_docs: æœ€å°‘æ–‡æ¡£æ•°é‡
        """
        self.min_similarity = min_similarity
        self.min_docs = min_docs
        
        print(f"ğŸ›¡ï¸  åå¹»è§‰å®ˆå«å·²å¯ç”¨")
        print(f"   æœ€ä½ç›¸ä¼¼åº¦ï¼š{min_similarity}")
        print(f"   æœ€å°‘æ–‡æ¡£æ•°ï¼š{min_docs}")
    
    def check_retrieval_quality(self, docs: List[Document], query: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ£€ç´¢è´¨é‡
        
        Returns:
            {
                "quality": "good" | "medium" | "poor",
                "confidence": 0.0-1.0,
                "warning": str,
                "should_answer": bool
            }
        """
        # 1. æ£€æŸ¥æ–‡æ¡£æ•°é‡
        if not docs or len(docs) == 0:
            return {
                "quality": "poor",
                "confidence": 0.0,
                "warning": "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹",
                "should_answer": False
            }
        
        if len(docs) < self.min_docs:
            return {
                "quality": "poor",
                "confidence": 0.3,
                "warning": f"ç›¸å…³å†…å®¹è¾ƒå°‘ï¼ˆä»… {len(docs)} ä¸ªç‰‡æ®µï¼‰",
                "should_answer": False
            }
        
        # 2. æ£€æŸ¥ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
        max_similarity = self._get_max_similarity(docs)
        
        if max_similarity < self.min_similarity:
            return {
                "quality": "poor",
                "confidence": max_similarity,
                "warning": f"ç›¸å…³åº¦è¾ƒä½ï¼ˆ{max_similarity:.2f}ï¼‰",
                "should_answer": False
            }
        
        # 3. æ£€æŸ¥å†…å®¹é•¿åº¦
        total_length = sum(len(doc.page_content) for doc in docs)
        if total_length < 100:
            return {
                "quality": "poor",
                "confidence": 0.4,
                "warning": "ç›¸å…³å†…å®¹è¿‡å°‘",
                "should_answer": False
            }
        
        # 4. è¯„ä¼°æ•´ä½“è´¨é‡
        if max_similarity >= 0.8 and len(docs) >= 3:
            return {
                "quality": "good",
                "confidence": max_similarity,
                "warning": None,
                "should_answer": True
            }
        elif max_similarity >= 0.6 and len(docs) >= 2:
            return {
                "quality": "medium",
                "confidence": max_similarity,
                "warning": "ç›¸å…³å†…å®¹æœ‰é™ï¼Œç­”æ¡ˆå¯èƒ½ä¸å®Œæ•´",
                "should_answer": True
            }
        else:
            return {
                "quality": "poor",
                "confidence": max_similarity,
                "warning": "ç›¸å…³å†…å®¹ä¸è¶³",
                "should_answer": False
            }
    
    def build_anti_hallucination_prompt(
        self, 
        query: str, 
        docs: List[Document],
        quality_check: Dict[str, Any]
    ) -> str:
        """
        æ„å»ºåå¹»è§‰æç¤ºè¯
        
        ç­–ç•¥ï¼š
        1. æ˜ç¡®å‘ŠçŸ¥ LLM åªèƒ½åŸºäºæä¾›çš„å†…å®¹å›ç­”
        2. è¦æ±‚ LLM æ‰¿è®¤ä¸çŸ¥é“
        3. ç¦æ­¢ç¼–é€ ä¿¡æ¯
        4. è¦æ±‚å¼•ç”¨æ¥æº
        """
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # æ ¹æ®è´¨é‡è°ƒæ•´æç¤ºè¯
        if not quality_check["should_answer"]:
            # è´¨é‡å·®ï¼šæ˜ç¡®å‘ŠçŸ¥æ— æ³•å›ç­”
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¯šå®çš„æ™ºèƒ½åŠ©æ‰‹ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼š
{context if context else "ï¼ˆæœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼‰"}

âš ï¸ é‡è¦æç¤ºï¼š
- çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„ç›¸å…³å†…å®¹
- ä½ å¿…é¡»è¯šå®åœ°å‘Šè¯‰ç”¨æˆ·"æˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
- ç»å¯¹ä¸è¦ç¼–é€ æˆ–çŒœæµ‹ç­”æ¡ˆ
- ä¸è¦ä½¿ç”¨ä½ çš„è®­ç»ƒæ•°æ®ä¸­çš„çŸ¥è¯†

è¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
        
        elif quality_check["quality"] == "medium":
            # è´¨é‡ä¸­ç­‰ï¼šè°¨æ…å›ç­”
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¯šå®çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ã€‚

çŸ¥è¯†åº“å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

âš ï¸ é‡è¦è§„åˆ™ï¼š
1. åªèƒ½ä½¿ç”¨ä¸Šè¿°çŸ¥è¯†åº“å†…å®¹å›ç­”
2. å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸è¶³ä»¥å®Œæ•´å›ç­”ï¼Œæ˜ç¡®è¯´æ˜"æ ¹æ®ç°æœ‰èµ„æ–™..."
3. ä¸è¦ç¼–é€ ä»»ä½•ä¸åœ¨çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯
4. ä¸è¦ä½¿ç”¨ä½ çš„è®­ç»ƒæ•°æ®è¡¥å……ç­”æ¡ˆ
5. å¦‚æœä¸ç¡®å®šï¼Œè¯´"æˆ‘ä¸ç¡®å®š"è€Œä¸æ˜¯çŒœæµ‹

è¯·åŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ã€‚"""
        
        else:
            # è´¨é‡å¥½ï¼šæ­£å¸¸å›ç­”ï¼Œä½†ä»è¦çº¦æŸ
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

çŸ¥è¯†åº“å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

é‡è¦è§„åˆ™ï¼š
1. ä¸»è¦åŸºäºä¸Šè¿°çŸ¥è¯†åº“å†…å®¹å›ç­”
2. å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸è¶³ï¼Œå¯ä»¥é€‚å½“è¡¥å……å¸¸è¯†ï¼Œä½†è¦æ˜ç¡®è¯´æ˜
3. ä¸è¦ç¼–é€ å…·ä½“çš„æ•°å­—ã€æ—¥æœŸã€äººåç­‰ç»†èŠ‚
4. ä¿æŒå®¢è§‚å’Œå‡†ç¡®

è¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
        
        return prompt
    
    def validate_answer(
        self, 
        answer: str, 
        docs: List[Document],
        query: str
    ) -> Dict[str, Any]:
        """
        éªŒè¯ç­”æ¡ˆè´¨é‡ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        
        æ£€æŸ¥ï¼š
        1. ç­”æ¡ˆæ˜¯å¦æ‰¿è®¤ä¸çŸ¥é“
        2. ç­”æ¡ˆé•¿åº¦æ˜¯å¦åˆç†
        3. æ˜¯å¦åŒ…å«è­¦å‘Šè¯
        """
        answer_lower = answer.lower()
        
        # æ£€æŸ¥æ˜¯å¦æ‰¿è®¤ä¸çŸ¥é“
        unknown_phrases = [
            "ä¸çŸ¥é“", "æ²¡æœ‰æ‰¾åˆ°", "æ— æ³•ç¡®å®š", "ä¸ç¡®å®š",
            "æ²¡æœ‰ç›¸å…³", "æœªæ‰¾åˆ°", "æ— æ³•å›ç­”", "ä¸æ¸…æ¥š"
        ]
        admits_unknown = any(phrase in answer_lower for phrase in unknown_phrases)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼–é€ è¿¹è±¡ï¼ˆè¿‡äºå…·ä½“çš„æ•°å­—ã€æ—¥æœŸç­‰ï¼‰
        # è¿™æ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…å¯ä»¥æ›´å¤æ‚
        suspicious_patterns = [
            "æ‰‹æœºå·", "ç”µè¯", "é‚®ç®±", "qq", "å¾®ä¿¡",
            "èº«ä»½è¯", "é“¶è¡Œå¡"
        ]
        has_suspicious = any(pattern in answer_lower for pattern in suspicious_patterns)
        
        # æ£€æŸ¥ç­”æ¡ˆé•¿åº¦
        if len(answer) < 10:
            quality = "poor"
        elif admits_unknown and len(docs) < 2:
            quality = "good"  # æ­£ç¡®æ‰¿è®¤ä¸çŸ¥é“
        elif has_suspicious:
            quality = "suspicious"  # å¯èƒ½æœ‰å¹»è§‰
        else:
            quality = "good"
        
        return {
            "quality": quality,
            "admits_unknown": admits_unknown,
            "has_suspicious": has_suspicious,
            "length": len(answer)
        }
    
    def _get_max_similarity(self, docs: List[Document]) -> float:
        """è·å–æœ€é«˜ç›¸ä¼¼åº¦"""
        if not docs:
            return 0.0
        
        scores = []
        for doc in docs:
            score = doc.metadata.get("score", 0.0)
            if score > 0:
                scores.append(score)
        
        if scores:
            return max(scores)
        
        # å¦‚æœæ²¡æœ‰åˆ†æ•°ï¼Œæ ¹æ®æ–‡æ¡£æ•°é‡ä¼°ç®—
        return 0.8 if len(docs) >= 3 else 0.6
    
    def get_fallback_response(self, query: str, warning: str) -> str:
        """
        è·å–é™çº§å“åº”ï¼ˆå½“æ£€ç´¢è´¨é‡å¤ªå·®æ—¶ï¼‰
        """
        return f"""æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°å…³äº"{query}"çš„ç›¸å…³ä¿¡æ¯ã€‚

åŸå› ï¼š{warning}

å»ºè®®ï¼š
1. å°è¯•æ¢ä¸€ç§é—®æ³•
2. ç¡®è®¤é—®é¢˜æ˜¯å¦åœ¨çŸ¥è¯†åº“èŒƒå›´å†…
3. æ£€æŸ¥æ˜¯å¦æœ‰æ‹¼å†™é”™è¯¯

æˆ‘åªèƒ½åŸºäºå·²å¯¼å…¥çš„æ–‡æ¡£å›ç­”é—®é¢˜ï¼Œæ— æ³•ç¼–é€ æˆ–çŒœæµ‹ç­”æ¡ˆã€‚"""


class CitationEnforcer:
    """å¼•ç”¨å¼ºåˆ¶å™¨ - è¦æ±‚ LLM å¼•ç”¨æ¥æº"""
    
    @staticmethod
    def add_citation_requirement(prompt: str, docs: List[Document]) -> str:
        """
        åœ¨æç¤ºè¯ä¸­æ·»åŠ å¼•ç”¨è¦æ±‚
        """
        # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ ç¼–å·
        numbered_context = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get("source", "æœªçŸ¥")
            page = doc.metadata.get("page", "æœªçŸ¥")
            numbered_context.append(
                f"[æ–‡æ¡£{i}]ï¼ˆæ¥æºï¼š{source}ï¼Œé¡µç ï¼š{page}ï¼‰\n{doc.page_content}"
            )
        
        context = "\n\n".join(numbered_context)
        
        # ä¿®æ”¹æç¤ºè¯ï¼Œè¦æ±‚å¼•ç”¨
        enhanced_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ç¼–å·çš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚

{context}

ç”¨æˆ·é—®é¢˜ï¼š{prompt.split('ç”¨æˆ·é—®é¢˜ï¼š')[-1] if 'ç”¨æˆ·é—®é¢˜ï¼š' in prompt else prompt}

é‡è¦è¦æ±‚ï¼š
1. å›ç­”æ—¶å¿…é¡»å¼•ç”¨æ–‡æ¡£ç¼–å·ï¼Œå¦‚"æ ¹æ®[æ–‡æ¡£1]..."
2. ä¸åŒæ¥æºçš„ä¿¡æ¯è¦åˆ†åˆ«è¯´æ˜
3. å¦‚æœå¤šä¸ªæ–‡æ¡£æœ‰çŸ›ç›¾ï¼Œè¦æŒ‡å‡ºå·®å¼‚
4. åªä½¿ç”¨ä¸Šè¿°æ–‡æ¡£ä¸­çš„ä¿¡æ¯

è¯·å›ç­”é—®é¢˜ï¼Œå¹¶æ ‡æ³¨å¼•ç”¨æ¥æºã€‚"""
        
        return enhanced_prompt
    
    @staticmethod
    def extract_citations(answer: str) -> List[int]:
        """æå–ç­”æ¡ˆä¸­çš„å¼•ç”¨ç¼–å·"""
        import re
        citations = re.findall(r'\[æ–‡æ¡£(\d+)\]', answer)
        return [int(c) for c in citations]


class FactChecker:
    """äº‹å®æ£€æŸ¥å™¨ - æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦ä¸æ–‡æ¡£ä¸€è‡´"""
    
    @staticmethod
    def check_consistency(answer: str, docs: List[Document]) -> Dict[str, Any]:
        """
        æ£€æŸ¥ç­”æ¡ˆä¸æ–‡æ¡£çš„ä¸€è‡´æ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        
        å®é™…åº”è¯¥ç”¨æ›´å¤æ‚çš„ NLP æŠ€æœ¯
        """
        # æå–æ–‡æ¡£ä¸­çš„å…³é”®å®ä½“
        doc_entities = set()
        for doc in docs:
            # ç®€å•æå–ï¼šä¸­æ–‡äººåã€åœ°åç­‰ï¼ˆå®é™…åº”è¯¥ç”¨ NERï¼‰
            content = doc.page_content
            doc_entities.update(content.split())
        
        # æ£€æŸ¥ç­”æ¡ˆä¸­çš„å®ä½“æ˜¯å¦åœ¨æ–‡æ¡£ä¸­
        answer_words = set(answer.split())
        
        # è®¡ç®—é‡å åº¦
        if not answer_words:
            overlap = 0.0
        else:
            overlap = len(answer_words & doc_entities) / len(answer_words)
        
        return {
            "overlap": overlap,
            "is_consistent": overlap > 0.3,  # 30% é‡å è®¤ä¸ºä¸€è‡´
            "doc_entities_count": len(doc_entities),
            "answer_words_count": len(answer_words)
        }
