import os
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
try:
    from langchain_chroma import Chroma
except ImportError:
    from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import DashScopeEmbeddings
from dotenv import load_dotenv

load_dotenv()

class RAGManager:
    def __init__(self, data_dir="data", persist_dir="vector_store"):
        self.data_dir = data_dir
        self.persist_dir = persist_dir
        # ä½¿ç”¨é˜¿é‡Œäº‘çš„ Embedding æ¨¡å‹
        self.embeddings = DashScopeEmbeddings(
            model=os.getenv("EMBEDDING_MODEL", "text-embedding-v3")
        )
        self.vector_store = None

    def load_and_index(self):
        """åŠ è½½ data ç›®å½•ä¸‹çš„æ–‡æ¡£å¹¶å»ºç«‹ç´¢å¼•"""
        print(f"Loading documents from {self.data_dir}...")
        
        from langchain_community.document_loaders import TextLoader, PyPDFLoader, UnstructuredEPubLoader
        
        # æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
        loaders = [
            DirectoryLoader(self.data_dir, glob="**/*.pdf", loader_cls=PyPDFLoader),
            DirectoryLoader(self.data_dir, glob="**/*.txt", loader_cls=TextLoader),
            DirectoryLoader(self.data_dir, glob="**/*.md", loader_cls=TextLoader),
            DirectoryLoader(self.data_dir, glob="**/*.epub", loader_cls=UnstructuredEPubLoader),
        ]
        
        documents = []
        for loader in loaders:
            try:
                docs = loader.load()
                documents.extend(docs)
                if docs:
                    print(f"  âœ… Loaded {len(docs)} documents from {loader.glob}")
            except Exception as e:
                print(f"  âš ï¸  Warning loading {loader.glob}: {e}")
        
        if not documents:
            print("âŒ No documents found.")
            return

        print(f"\nğŸ“š Total documents loaded: {len(documents)}")

        # æ–‡æœ¬åˆ‡ç‰‡
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.split_documents(documents)
        print(f"âœ‚ï¸  Split into {len(texts)} chunks.")

        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        self.vector_store = Chroma.from_documents(
            documents=texts,
            embedding=self.embeddings,
            persist_directory=self.persist_dir
        )
        print("âœ… Indexing completed and persisted.")

    def get_retriever(self):
        """è·å–æ£€ç´¢å™¨"""
        if not self.vector_store:
            # å¦‚æœå†…å­˜é‡Œæ²¡æœ‰ï¼Œå°è¯•ä»æœ¬åœ°åŠ è½½
            self.vector_store = Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self.embeddings
            )
        return self.vector_store.as_retriever(search_kwargs={"k": 3})
