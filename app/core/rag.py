"""
RAG Manager - æ”¯æŒå…è´¹ Embedding æ¨¡å‹
"""
import os
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
try:
    from langchain_chroma import Chroma
except ImportError:
    from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv

load_dotenv()

class RAGManager:
    def __init__(self, data_dir="data", persist_dir="vector_store"):
        self.data_dir = data_dir
        self.persist_dir = persist_dir
        self.embeddings = self._get_embeddings()
        self.vector_store = None

    def _get_embeddings(self):
        """
        è·å– Embedding æ¨¡å‹
        
        æ”¯æŒçš„ç±»å‹ï¼š
        - aliyun: é˜¿é‡Œäº‘ text-embedding-v3ï¼ˆä»˜è´¹ï¼‰
        - local: æœ¬åœ° HuggingFace æ¨¡å‹ï¼ˆå…è´¹ï¼‰
        """
        embedding_type = os.getenv("EMBEDDING_TYPE", "local").lower()
        
        if embedding_type == "aliyun":
            from langchain_community.embeddings import DashScopeEmbeddings
            print("ğŸ“Š ä½¿ç”¨é˜¿é‡Œäº‘ Embedding: text-embedding-v3")
            print("ğŸ’° æŒ‰ Token è®¡è´¹")
            return DashScopeEmbeddings(
                model=os.getenv("EMBEDDING_MODEL", "text-embedding-v3")
            )
        
        elif embedding_type == "local":
            from langchain_community.embeddings import HuggingFaceEmbeddings
            model_name = os.getenv("LOCAL_EMBEDDING_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            print(f"ğŸ“Š ä½¿ç”¨æœ¬åœ° Embedding: {model_name}")
            print("ğŸ’° å®Œå…¨å…è´¹ï¼Œæ— éœ€ API Key")
            print("â³ é¦–æ¬¡ä½¿ç”¨ä¼šä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 500MBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            return HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ Embedding ç±»å‹: {embedding_type}")

    def load_and_index(self):
        """åŠ è½½ data ç›®å½•ä¸‹çš„æ–‡æ¡£å¹¶å»ºç«‹ç´¢å¼•"""
        print(f"Loading documents from {self.data_dir}...")
        
        from langchain_community.document_loaders import TextLoader, PyPDFLoader, UnstructuredEPubLoader
        
        # æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
        loaders = [
            DirectoryLoader(self.data_dir, glob="**/*.pdf", loader_cls=PyPDFLoader),
            DirectoryLoader(self.data_dir, glob="**/*.txt", loader_cls=TextLoader),
            DirectoryLoader(self.data_dir, glob="**/*.md", loader_cls=TextLoader),
            DirectoryLoader(self.data_dir, glob="**/*.epub", loader_cls=UnstructuredEPubLoader),
        ]
        
        documents = []
        for loader in loaders:
            try:
                docs = loader.load()
                documents.extend(docs)
                if docs:
                    print(f"  âœ… Loaded {len(docs)} documents from {loader.glob}")
            except Exception as e:
                print(f"  âš ï¸  Warning loading {loader.glob}: {e}")
        
        if not documents:
            print("âŒ No documents found.")
            return

        print(f"\nğŸ“š Total documents loaded: {len(documents)}")

        # æ–‡æœ¬åˆ‡ç‰‡
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.split_documents(documents)
        print(f"âœ‚ï¸  Split into {len(texts)} chunks.")

        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        self.vector_store = Chroma.from_documents(
            documents=texts,
            embedding=self.embeddings,
            persist_directory=self.persist_dir
        )
        print("âœ… Indexing completed and persisted.")

    def get_retriever(self):
        """è·å–æ£€ç´¢å™¨"""
        if not self.vector_store:
            # å¦‚æœå†…å­˜é‡Œæ²¡æœ‰ï¼Œå°è¯•ä»æœ¬åœ°åŠ è½½
            self.vector_store = Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self.embeddings
            )
        return self.vector_store.as_retriever(search_kwargs={"k": 3})
