"""
RAG Manager - æ”¯æŒå…è´¹ Embedding æ¨¡å‹å’Œæ–‡æ¡£æ ‡ç­¾
"""
import os
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
try:
    from langchain_chroma import Chroma
except ImportError:
    from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv
from app.core.document_tagger import DocumentTagger

load_dotenv()

class RAGManager:
    def __init__(self, data_dir="data", persist_dir="vector_store"):
        self.data_dir = data_dir
        self.persist_dir = persist_dir
        self.embeddings = self._get_embeddings()
        self.vector_store = None
        self.tagger = DocumentTagger()  # æ–°å¢ï¼šæ–‡æ¡£æ ‡ç­¾ç®¡ç†å™¨

    def _get_embeddings(self):
        """
        è·å– Embedding æ¨¡å‹ï¼ˆæœ¬åœ° HuggingFace æ¨¡å‹ï¼‰
        """
        from langchain_community.embeddings import HuggingFaceEmbeddings
        model_name = os.getenv("LOCAL_EMBEDDING_MODEL", "BAAI/bge-large-zh-v1.5")
        print(f"ğŸ“Š ä½¿ç”¨æœ¬åœ° Embedding: {model_name}")
        print("ğŸ’° å®Œå…¨å…è´¹ï¼Œæ— éœ€ API Key")
        print("â³ é¦–æ¬¡ä½¿ç”¨ä¼šä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 500MBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        return HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

    def load_and_index(self):
        """åŠ è½½ data ç›®å½•ä¸‹çš„æ–‡æ¡£å¹¶å»ºç«‹ç´¢å¼•ï¼ˆå¸¦æ ‡ç­¾ï¼‰"""
        print(f"Loading documents from {self.data_dir}...")
        
        from langchain_community.document_loaders import TextLoader, PyPDFLoader, UnstructuredEPubLoader
        
        # æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
        loaders = [
            DirectoryLoader(self.data_dir, glob="**/*.pdf", loader_cls=PyPDFLoader),
            DirectoryLoader(self.data_dir, glob="**/*.txt", loader_cls=TextLoader),
            DirectoryLoader(self.data_dir, glob="**/*.md", loader_cls=TextLoader),
            DirectoryLoader(self.data_dir, glob="**/*.epub", loader_cls=UnstructuredEPubLoader),
        ]
        
        documents = []
        for loader in loaders:
            try:
                docs = loader.load()
                documents.extend(docs)
                if docs:
                    print(f"  âœ… Loaded {len(docs)} documents from {loader.glob}")
            except Exception as e:
                print(f"  âš ï¸  Warning loading {loader.glob}: {e}")
        
        if not documents:
            print("âŒ No documents found.")
            return

        print(f"\nğŸ“š Total documents loaded: {len(documents)}")
        
        # ä¸ºæ–‡æ¡£æ·»åŠ æ ‡ç­¾
        print(f"ğŸ·ï¸  Adding tags to documents...")
        for doc in documents:
            source = doc.metadata.get("source", "")
            tags = self.tagger.get_tags_for_file(source)
            
            # æ‰å¹³åŒ–æ ‡ç­¾æ•°æ®ï¼ˆChroma ä¸æ”¯æŒåµŒå¥—å­—å…¸å’Œåˆ—è¡¨ï¼‰
            doc.metadata["book"] = tags.get("book", "æœªçŸ¥")
            doc.metadata["author"] = tags.get("author", "æœªçŸ¥")
            doc.metadata["dynasty"] = tags.get("dynasty", "æœªçŸ¥")
            doc.metadata["genre"] = tags.get("genre", "æœªçŸ¥")
            
            # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            if "category" in tags and isinstance(tags["category"], list):
                doc.metadata["category"] = ", ".join(tags["category"])
            else:
                doc.metadata["category"] = tags.get("category", "å…¶ä»–")
            
            if "keywords" in tags and isinstance(tags["keywords"], list):
                doc.metadata["keywords"] = ", ".join(tags["keywords"])
            else:
                doc.metadata["keywords"] = ""
        
        # æ‰“å°æ ‡ç­¾ç»Ÿè®¡
        tag_stats = {}
        for doc in documents:
            book = doc.metadata.get("book", "æœªçŸ¥")
            tag_stats[book] = tag_stats.get(book, 0) + 1
        
        print(f"ğŸ“Š Documents by book:")
        for book, count in tag_stats.items():
            print(f"  - {book}: {count} documents")

        # æ–‡æœ¬åˆ‡ç‰‡
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.split_documents(documents)
        print(f"âœ‚ï¸  Split into {len(texts)} chunks.")

        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        self.vector_store = Chroma.from_documents(
            documents=texts,
            embedding=self.embeddings,
            persist_directory=self.persist_dir
        )
        print("âœ… Indexing completed and persisted.")
        print(f"ğŸ’¡ æ‰€æœ‰æ–‡æ¡£å·²æ·»åŠ æ ‡ç­¾ï¼Œå¯ä»¥ä½¿ç”¨æ ‡ç­¾è¿‡æ»¤æ£€ç´¢ç»“æœ")

    def get_retriever(self, k=5, filters=None):
        """
        è·å–æ£€ç´¢å™¨
        
        å‚æ•°:
            k: æ£€ç´¢æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ 5ï¼ˆå¢åŠ æ£€ç´¢æ•°é‡å¯æé«˜å¬å›ç‡ï¼‰
            filters: æ ‡ç­¾è¿‡æ»¤æ¡ä»¶ï¼Œå¦‚ {"book": "çº¢æ¥¼æ¢¦"}
        """
        if not self.vector_store:
            # å¦‚æœå†…å­˜é‡Œæ²¡æœ‰ï¼Œå°è¯•ä»æœ¬åœ°åŠ è½½
            self.vector_store = Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self.embeddings
            )
        
        # æ„å»ºæ£€ç´¢å‚æ•°
        search_kwargs = {"k": k}
        
        # å¦‚æœæœ‰è¿‡æ»¤æ¡ä»¶ï¼Œæ·»åŠ åˆ°æ£€ç´¢å‚æ•°
        if filters:
            # Chroma ä½¿ç”¨ where å‚æ•°è¿›è¡Œå…ƒæ•°æ®è¿‡æ»¤
            # ä¾‹å¦‚: {"book": "çº¢æ¥¼æ¢¦"}
            search_kwargs["filter"] = filters
        
        return self.vector_store.as_retriever(search_kwargs=search_kwargs)
    
    def search_by_book(self, query: str, book_name: str, k=5):
        """
        æŒ‰ä¹¦åæ£€ç´¢
        
        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            book_name: ä¹¦åï¼ˆå¦‚ "çº¢æ¥¼æ¢¦"ï¼‰
            k: è¿”å›æ–‡æ¡£æ•°é‡
        """
        if not self.vector_store:
            self.vector_store = Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self.embeddings
            )
        
        # ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤
        results = self.vector_store.similarity_search(
            query,
            k=k,
            filter={"book": book_name}
        )
        
        return results
    
    def get_books_list(self):
        """è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰ä¹¦ç±"""
        return self.tagger.get_books()
    
    def get_tag_statistics(self):
        """è·å–æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯"""
        return self.tagger.get_statistics()
