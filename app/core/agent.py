import os
from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from app.core.rag import RAGManager
from app.core.tools import get_all_tools
from dotenv import load_dotenv

load_dotenv()

class SimpleMemory:
    """ç®€å•çš„å¯¹è¯è®°å¿†ï¼Œä¿ç•™æœ€è¿‘ k è½®å¯¹è¯"""
    def __init__(self, k=5):
        self.k = k
        self.messages = []
    
    def save_context(self, inputs, outputs):
        """ä¿å­˜å¯¹è¯"""
        self.messages.append(HumanMessage(content=inputs["input"]))
        self.messages.append(AIMessage(content=outputs["output"]))
        # åªä¿ç•™æœ€è¿‘ k è½®ï¼ˆk*2 æ¡æ¶ˆæ¯ï¼‰
        if len(self.messages) > self.k * 2:
            self.messages = self.messages[-(self.k * 2):]
    
    def load_memory_variables(self, inputs=None):
        """åŠ è½½è®°å¿†"""
        return {"chat_history": self.messages}
    
    def clear(self):
        """æ¸…ç©ºè®°å¿†"""
        self.messages = []


class AgentManager:
    def __init__(self, session_id: str = "default"):
        # è·å–æ¨¡å‹æä¾›å•†é…ç½®
        provider = os.getenv("MODEL_PROVIDER", "aliyun").lower()
        self.provider = provider
        self.session_id = session_id
        
        # æ ¹æ®æä¾›å•†åˆå§‹åŒ– LLM
        if provider == "groq":
            self.llm = ChatOpenAI(
                model=os.getenv("GROQ_LLM_MODEL", "llama-3.3-70b-versatile"),
                openai_api_base="https://api.groq.com/openai/v1",
                openai_api_key=os.getenv("GROQ_API_KEY")
            )
            print(f"âœ… ä½¿ç”¨ Groq æ¨¡å‹: {os.getenv('GROQ_LLM_MODEL', 'llama-3.3-70b-versatile')}")
            print("â„¹ï¸  Groq ä½¿ç”¨ç®€åŒ–çš„ RAG æ¨¡å¼ï¼ˆä¸ä½¿ç”¨ Agentï¼‰")
        else:  # é»˜è®¤ä½¿ç”¨é˜¿é‡Œäº‘
            self.llm = ChatOpenAI(
                model=os.getenv("LLM_MODEL", "qwen-plus"),
                openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
                openai_api_key=os.getenv("DASHSCOPE_API_KEY")
            )
            print(f"âœ… ä½¿ç”¨é˜¿é‡Œäº‘æ¨¡å‹: {os.getenv('LLM_MODEL', 'qwen-plus')}")
        
        self.rag = RAGManager()
        
        # æ–°å¢ï¼šè®°å¿†ç³»ç»Ÿï¼ˆä¿ç•™æœ€è¿‘5è½®å¯¹è¯ï¼‰
        self.memory = SimpleMemory(k=5)
        
        # è®°å½•çŠ¶æ€
        self.last_call_info = {
            "mode": None,
            "llm_called": False,
            "tools_used": [],
            "keyword_matched": None,
        }
        self.last_retrieved_docs = []
        self.used_knowledge_base = False
        self.used_direct_retrieval = False
        
        # æ‰“å°å…³é”®è¯ç»Ÿè®¡
        print(f"ğŸ§  è®°å¿†ç³»ç»Ÿå·²å¯ç”¨ï¼ˆä¿ç•™æœ€è¿‘ 5 è½®å¯¹è¯ï¼‰")

    def create_agent(self, chat_history=None):
        # 1. åˆ›å»ºæ£€ç´¢å™¨
        retriever = self.rag.get_retriever()
        
        # 2. å®šä¹‰çŸ¥è¯†åº“æ£€ç´¢å·¥å…·
        @tool
        def search_knowledge_base(query: str) -> str:
            """æœç´¢æœ¬åœ°çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯ã€‚å¯¹äºä»»ä½•é—®é¢˜ï¼Œéƒ½åº”è¯¥å…ˆä½¿ç”¨æ­¤å·¥å…·æœç´¢çŸ¥è¯†åº“ï¼Œçœ‹æ˜¯å¦æœ‰ç›¸å…³å†…å®¹ã€‚çŸ¥è¯†åº“ä¸­åŒ…å«å››å¤§åè‘—ï¼ˆçº¢æ¥¼æ¢¦ã€ä¸‰å›½æ¼”ä¹‰ã€è¥¿æ¸¸è®°ã€æ°´æµ’ä¼ ï¼‰çš„å®Œæ•´å†…å®¹ã€‚"""
            docs = retriever.invoke(query)
            # è®°å½•æ£€ç´¢åˆ°çš„æ–‡æ¡£
            self.last_retrieved_docs = docs
            self.used_knowledge_base = True
            
            # é˜²å¹»è§‰æœºåˆ¶ï¼šå¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£ï¼Œæ˜ç¡®è¿”å›
            if not docs:
                return "ã€çŸ¥è¯†åº“æ£€ç´¢ç»“æœã€‘æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚è¯·å‘ŠçŸ¥ç”¨æˆ·ï¼šçŸ¥è¯†åº“ä¸­æ²¡æœ‰å…³äºè¿™ä¸ªé—®é¢˜çš„ä¿¡æ¯ã€‚"
            
            # é˜²å¹»è§‰æœºåˆ¶ï¼šå¦‚æœæ£€ç´¢ç»“æœå¤ªå°‘ï¼Œæ ‡æ³¨è­¦å‘Š
            if len(docs) < 2:
                return f"ã€çŸ¥è¯†åº“æ£€ç´¢ç»“æœã€‘ä»…æ‰¾åˆ°å°‘é‡ç›¸å…³å†…å®¹ï¼Œè¯·è°¨æ…å›ç­”ï¼š\n\n{docs[0].page_content}\n\nã€æ³¨æ„ã€‘å¦‚æœå†…å®¹ä¸è¶³ä»¥å®Œæ•´å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ã€‚"
            
            return "\n\n".join([d.page_content for d in docs])

        # 3. è·å–æ‰€æœ‰å·¥å…·ï¼ˆçŸ¥è¯†åº“æ£€ç´¢ + é€šç”¨å·¥å…·ï¼‰
        tools = [search_knowledge_base] + get_all_tools()
        
        print(f"ğŸ”§ Agent å·²åŠ è½½ {len(tools)} ä¸ªå·¥å…·ï¼š")
        for i, t in enumerate(tools, 1):
            print(f"   {i}. {t.name} - {t.description[:50]}...")

        # 4. åˆ›å»º ReAct æç¤ºè¯æ¨¡æ¿
        history_text = ""
        if chat_history:
            history_text = "ã€å¯¹è¯å†å²ã€‘\n"
            for msg in chat_history:
                role = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "AI"
                history_text += f"{role}: {msg.content}\n"
            history_text += "\næ³¨æ„ï¼šç†è§£å¯¹è¯å†å²ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œç‰¹åˆ«æ˜¯ä»£è¯ï¼ˆå¦‚\"ä»–\"ã€\"è¿™æœ¬ä¹¦\"ï¼‰çš„æŒ‡ä»£å…³ç³»ã€‚\n\n"

        # ä½¿ç”¨ ReAct æç¤ºè¯æ¨¡æ¿
        react_prompt = PromptTemplate.from_template("""ä½ æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ AI Agentï¼Œä¸“é—¨å›ç­”å…³äºä¸­å›½å››å¤§åè‘—çš„é—®é¢˜ã€‚

{history_text}ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·ï¼š

{tools}

ä½¿ç”¨ä»¥ä¸‹æ ¼å¼è¿›è¡Œæ¨ç†ï¼š

Question: ç”¨æˆ·çš„é—®é¢˜
Thought: ä½ åº”è¯¥æ€è€ƒè¦åšä»€ä¹ˆ
Action: è¦ä½¿ç”¨çš„å·¥å…·ï¼Œåº”è¯¥æ˜¯ [{tool_names}] ä¸­çš„ä¸€ä¸ª
Action Input: å·¥å…·çš„è¾“å…¥
Observation: å·¥å…·è¿”å›çš„ç»“æœ
... (è¿™ä¸ª Thought/Action/Action Input/Observation å¯ä»¥é‡å¤ N æ¬¡)
Thought: æˆ‘ç°åœ¨çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†
Final Answer: å¯¹ç”¨æˆ·é—®é¢˜çš„æœ€ç»ˆç­”æ¡ˆ

ã€é‡è¦è§„åˆ™ã€‘
1. å¯¹äºå››å¤§åè‘—çš„é—®é¢˜ï¼Œå¿…é¡»ä½¿ç”¨ search_knowledge_base å·¥å…·
2. åªèƒ½åŸºäºå·¥å…·è¿”å›çš„ç»“æœå›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
3. å¦‚æœå·¥å…·è¿”å›"æœªæ‰¾åˆ°"ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
4. ç»“åˆå¯¹è¯å†å²ç†è§£é—®é¢˜ä¸­çš„ä»£è¯æŒ‡ä»£

å¼€å§‹ï¼

Question: {input}
Thought: {agent_scratchpad}""")

        # 5. åˆ›å»º ReAct Agent
        agent = create_react_agent(
            llm=self.llm,
            tools=tools,
            prompt=react_prompt.partial(history_text=history_text)
        )
        
        # 6. åˆ›å»º AgentExecutor
        agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,  # æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
            max_iterations=5,  # æœ€å¤š5æ­¥æ¨ç†
            handle_parsing_errors=True,  # å¤„ç†è§£æé”™è¯¯
            return_intermediate_steps=True,  # è¿”å›ä¸­é—´æ­¥éª¤
        )
        
        print("ğŸ¯ ReAct Agent å·²åˆ›å»ºï¼Œverbose=True")
        
        return agent_executor

    def run_simple_rag(self, query: str):
        """ç®€åŒ–çš„ RAG å®ç°ï¼Œä¸ä½¿ç”¨ Agentï¼ˆé€‚ç”¨äº Groqï¼‰
        åŒ…å«é˜²å¹»è§‰æœºåˆ¶å’Œå¯¹è¯å†å²"""
        # é‡ç½®çŠ¶æ€
        self.last_retrieved_docs = []
        self.used_knowledge_base = False
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retriever = self.rag.get_retriever()
        docs = retriever.invoke(query)
        self.last_retrieved_docs = docs
        
        # 2. è·å–å¯¹è¯å†å²
        memory_vars = self.memory.load_memory_variables({})
        chat_history = memory_vars.get("chat_history", [])
        history_text = ""
        if chat_history:
            history_text = "\nã€å¯¹è¯å†å²ã€‘\n"
            for msg in chat_history:
                role = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "AI"
                history_text += f"{role}: {msg.content}\n"
            history_text += "\n"
        
        # 3. æ„å»ºæç¤ºè¯ï¼ˆé˜²å¹»è§‰ä¼˜åŒ–ï¼‰
        if not docs:
            # æ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚

{history_text}ã€é‡è¦æç¤ºã€‘
çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”ã€‘
æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚è¯·å°è¯•æ¢ä¸€ä¸ªé—®é¢˜æˆ–æä¾›æ›´å¤šä¸Šä¸‹æ–‡ã€‚"""
        elif len(docs) < 2:
            # æ£€ç´¢ç»“æœå¤ªå°‘
            self.used_knowledge_base = True
            context = docs[0].page_content
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚

{history_text}ã€é‡è¦æç¤ºã€‘
çŸ¥è¯†åº“ä¸­åªæ‰¾åˆ°äº†å°‘é‡ç›¸å…³å†…å®¹ï¼Œè¯·è°¨æ…å›ç­”ã€‚

ã€çŸ¥è¯†åº“å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- åªåŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”
- å¦‚æœå†…å®¹ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
- ä¸è¦ç¼–é€ æˆ–æ¨æµ‹
- æ³¨æ„å¯¹è¯å†å²ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œç†è§£ä»£è¯æŒ‡ä»£"""
        else:
            # æ­£å¸¸æƒ…å†µ
            self.used_knowledge_base = True
            context = "\n\n".join([doc.page_content for doc in docs])
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

{history_text}ã€æ ¸å¿ƒè§„åˆ™ã€‘
1. åªèƒ½åŸºäºä¸‹æ–¹æä¾›çš„çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜
2. å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œæ˜ç¡®è¯´æ˜
3. ä¸è¦ç¼–é€ ã€æ¨æµ‹æˆ–ä½¿ç”¨çŸ¥è¯†åº“å¤–çš„ä¿¡æ¯
4. å¼•ç”¨åŸæ–‡æ—¶è¦å‡†ç¡®ï¼Œä¸è¦ç¯¡æ”¹æˆ–è¿‡åº¦è§£è¯»
5. æ³¨æ„å¯¹è¯å†å²ï¼Œç†è§£ä»£è¯ï¼ˆå¦‚\"ä»–\"ã€\"è¿™æœ¬ä¹¦\"ï¼‰çš„æŒ‡ä»£å…³ç³»

ã€çŸ¥è¯†åº“å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- å¦‚æœçŸ¥è¯†åº“ä¸­æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œè¯·è¯¦ç»†å›ç­”
- å¦‚æœçŸ¥è¯†åº“ä¸­åªæœ‰éƒ¨åˆ†ä¿¡æ¯ï¼Œè¯´æ˜"æ ¹æ®çŸ¥è¯†åº“å†…å®¹ï¼Œå¯ä»¥å›ç­”ä»¥ä¸‹éƒ¨åˆ†ï¼š..."
- å¦‚æœçŸ¥è¯†åº“ä¸­å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥å›ç­”"æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰å…³äºè¿™ä¸ªé—®é¢˜çš„ä¿¡æ¯"
- å›ç­”æ—¶å¯ä»¥é€‚å½“å¼•ç”¨åŸæ–‡ç‰‡æ®µï¼Œç”¨å¼•å·æ ‡æ³¨
- ç»“åˆå¯¹è¯å†å²ç†è§£é—®é¢˜ä¸­çš„ä»£è¯æŒ‡ä»£"""
        
        # 4. è°ƒç”¨ LLM
        
        # æ‰“å°å‘é€ç»™ LLM çš„ prompt
        print("\n" + "="*80)
        print("ğŸ“¤ å‘é€ç»™ LLM çš„ Prompt (Simple RAG)ï¼š")
        print("="*80)
        print(prompt)
        print("="*80 + "\n")
        
        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        
        return response.content

    def direct_retrieval(self, query: str) -> str:
        """
        ç›´æ¥æ£€ç´¢æ¨¡å¼ - å‘½ä¸­å…³é”®è¯åï¼Œæ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶é€šè¿‡ LLM ç”Ÿæˆç­”æ¡ˆ
        è¿™æ˜¯ä¼˜åŒ–åçš„ RAG æµç¨‹ï¼šè·³è¿‡ Agent å·¥å…·è°ƒç”¨ï¼Œç›´æ¥æ£€ç´¢ + LLM ç”Ÿæˆ
        åŒ…å«é˜²å¹»è§‰æœºåˆ¶ï¼šç›¸ä¼¼åº¦é˜ˆå€¼ã€çŸ¥è¯†åº“è¦†ç›–ç‡æ£€æŸ¥ã€å¯¹è¯å†å²
        """
        # é‡ç½®çŠ¶æ€
        self.last_retrieved_docs = []
        self.used_knowledge_base = True
        self.used_direct_retrieval = True
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retriever = self.rag.get_retriever()
        docs = retriever.invoke(query)
        self.last_retrieved_docs = docs
        
        # 2. è·å–å¯¹è¯å†å²
        memory_vars = self.memory.load_memory_variables({})
        chat_history = memory_vars.get("chat_history", [])
        history_text = ""
        if chat_history:
            history_text = "\nã€å¯¹è¯å†å²ã€‘\n"
            for msg in chat_history:
                role = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "AI"
                history_text += f"{role}: {msg.content}\n"
            history_text += "\n"
        
        # 3. çŸ¥è¯†åº“è¦†ç›–ç‡æ£€æŸ¥ï¼ˆé˜²å¹»è§‰æœºåˆ¶ 1ï¼‰
        if not docs:
            # æ²¡æœ‰æ£€ç´¢åˆ°ä»»ä½•æ–‡æ¡£
            return "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚è¯·å°è¯•æ¢ä¸€ä¸ªé—®é¢˜æˆ–æä¾›æ›´å¤šä¸Šä¸‹æ–‡ã€‚"
        
        # 4. ç›¸ä¼¼åº¦é˜ˆå€¼æ£€æŸ¥ï¼ˆé˜²å¹»è§‰æœºåˆ¶ 2ï¼‰
        if len(docs) < 2:
            # æ£€ç´¢ç»“æœå¤ªå°‘ï¼Œå¯èƒ½ç›¸å…³æ€§ä¸é«˜
            context = docs[0].page_content
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚

{history_text}ã€é‡è¦æç¤ºã€‘
çŸ¥è¯†åº“ä¸­åªæ‰¾åˆ°äº†å°‘é‡ç›¸å…³å†…å®¹ï¼Œè¯·è°¨æ…å›ç­”ã€‚å¦‚æœå†…å®¹ä¸è¶³ä»¥å®Œæ•´å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

ã€çŸ¥è¯†åº“å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- å¦‚æœå†…å®¹è¶³å¤Ÿï¼Œè¯·åŸºäºçŸ¥è¯†åº“å›ç­”
- å¦‚æœå†…å®¹ä¸è¶³ï¼Œè¯·è¯´æ˜"çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯æœ‰é™ï¼Œåªèƒ½æä¾›ä»¥ä¸‹å†…å®¹ï¼š..."
- ä¸è¦ç¼–é€ æˆ–æ¨æµ‹çŸ¥è¯†åº“å¤–çš„ä¿¡æ¯
- æ³¨æ„å¯¹è¯å†å²ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œç†è§£ä»£è¯æŒ‡ä»£"""
            
            messages = [HumanMessage(content=prompt)]
            response = self.llm.invoke(messages)
            return response.content
        
        # 5. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆæ­£å¸¸æƒ…å†µï¼‰
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # 6. æ„å»ºå¢å¼ºæç¤ºè¯ï¼ˆRAG æ ¸å¿ƒ - é˜²æ­¢å¹»è§‰ï¼‰
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

{history_text}ã€æ ¸å¿ƒè§„åˆ™ã€‘
1. åªèƒ½åŸºäºä¸‹æ–¹æä¾›çš„çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜
2. å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œæ˜ç¡®è¯´æ˜"çŸ¥è¯†åº“ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜"
3. ä¸è¦ç¼–é€ ã€æ¨æµ‹æˆ–ä½¿ç”¨çŸ¥è¯†åº“å¤–çš„ä¿¡æ¯
4. å¼•ç”¨åŸæ–‡æ—¶è¦å‡†ç¡®ï¼Œä¸è¦ç¯¡æ”¹æˆ–è¿‡åº¦è§£è¯»
5. ä¿æŒå®¢è§‚ä¸­ç«‹ï¼Œä¸è¦æ·»åŠ ä¸ªäººè§‚ç‚¹
6. æ³¨æ„å¯¹è¯å†å²ï¼Œç†è§£ä»£è¯ï¼ˆå¦‚\"ä»–\"ã€\"è¿™æœ¬ä¹¦\"ï¼‰çš„æŒ‡ä»£å…³ç³»

ã€çŸ¥è¯†åº“å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- å¦‚æœçŸ¥è¯†åº“ä¸­æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œè¯·è¯¦ç»†å›ç­”
- å¦‚æœçŸ¥è¯†åº“ä¸­åªæœ‰éƒ¨åˆ†ä¿¡æ¯ï¼Œè¯´æ˜"æ ¹æ®çŸ¥è¯†åº“å†…å®¹ï¼Œå¯ä»¥å›ç­”ä»¥ä¸‹éƒ¨åˆ†ï¼š..."
- å¦‚æœçŸ¥è¯†åº“ä¸­å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥å›ç­”"æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰å…³äºè¿™ä¸ªé—®é¢˜çš„ä¿¡æ¯"
- å›ç­”æ—¶å¯ä»¥é€‚å½“å¼•ç”¨åŸæ–‡ç‰‡æ®µï¼Œç”¨å¼•å·æ ‡æ³¨
- ç»“åˆå¯¹è¯å†å²ç†è§£é—®é¢˜ä¸­çš„ä»£è¯æŒ‡ä»£"""
        
        # 7. è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        
        return response.content

    def run(self, query: str):
        # é‡ç½®è°ƒç”¨ä¿¡æ¯
        self.last_call_info = {
            "mode": None,
            "llm_called": False,
            "tools_used": [],
            "keyword_matched": None,
        }
        
        # Groq ä½¿ç”¨ç®€åŒ–çš„ RAGï¼Œé˜¿é‡Œäº‘ä½¿ç”¨ Agent
        if self.provider == "groq":
            print("ğŸ¤– ä½¿ç”¨ Groq ç®€åŒ– RAG æ¨¡å¼")
            self.last_call_info["mode"] = "simple_rag"
            self.last_call_info["llm_called"] = True
            answer = self.run_simple_rag(query)
        else:
            print("ğŸ¤– ä½¿ç”¨ Agent æ¨ç†æ¨¡å¼")
            self.last_call_info["mode"] = "agent"
            self.last_call_info["llm_called"] = True
            
            # é‡ç½®çŠ¶æ€
            self.last_retrieved_docs = []
            self.used_knowledge_base = False
            self.used_direct_retrieval = False
            
            # è·å–å¯¹è¯å†å²
            memory_vars = self.memory.load_memory_variables({})
            chat_history = memory_vars.get("chat_history", [])
            
            # åˆ›å»º ReAct Agentï¼ˆä¼ å…¥å¯¹è¯å†å²ï¼‰
            agent_executor = self.create_agent(chat_history=chat_history)
            
            # è°ƒç”¨ Agentï¼ˆReAct æ¨¡å¼ï¼‰
            print(f"ğŸš€ å¼€å§‹ ReAct æ¨ç†ï¼š{query}")
            print("="*60)
            
            # è°ƒç”¨ Agent
            result = agent_executor.invoke({"input": query})
            
            print("="*60)
            print("âœ… ReAct æ¨ç†å®Œæˆ")
            
            answer = result.get("output", "æœªèƒ½ç”Ÿæˆå›å¤ã€‚")
            
            
            # ä» intermediate_steps ä¸­æå–å›¾ç‰‡è·¯å¾„å¹¶æ·»åŠ åˆ°ç­”æ¡ˆ
            print(f"\nğŸ” DEBUG: æ£€æŸ¥ intermediate_steps")
            if "intermediate_steps" in result:
                print(f"   æ‰¾åˆ° {len(result['intermediate_steps'])} ä¸ªæ­¥éª¤")
                for i, (action, observation) in enumerate(result['intermediate_steps']):
                    print(f"   æ­¥éª¤ {i+1}: {action.tool}")
                    if isinstance(observation, str) and "[IMAGE_PATH:" in observation:
                        import re
                        image_match = re.search(r'\[IMAGE_PATH:(.*?)\]', observation)
                        if image_match:
                            image_path = image_match.group(1).strip()
                            answer = answer + f"\n\n[IMAGE_PATH:{image_path}]"
                            print(f"   âœ… å·²æ·»åŠ  IMAGE_PATH: {image_path}")
                            break
            
            # è®°å½•ä½¿ç”¨çš„å·¥å…·
            if "intermediate_steps" in result:
                print(f"\nğŸ“Š æ¨ç†æ­¥éª¤æ•°ï¼š{len(result['intermediate_steps'])}")
                for i, (action, observation) in enumerate(result['intermediate_steps'], 1):
                    tool_name = action.tool
                    self.last_call_info["tools_used"].append(tool_name)
                    print(f"\n  æ­¥éª¤ {i}:")
                    print(f"    ğŸ”§ å·¥å…·: {tool_name}")
                    print(f"    ğŸ“¥ è¾“å…¥: {action.tool_input}")
                    obs_preview = str(observation)[:200] + "..." if len(str(observation)) > 200 else str(observation)
                    print(f"    ğŸ“¤ ç»“æœ: {obs_preview}")

        
        # ä¿å­˜åˆ°è®°å¿†
        self.memory.save_context(
            {"input": query},
            {"output": answer}
        )
        
        # è·å–å½“å‰è®°å¿†ä¸­çš„æ¶ˆæ¯æ•°é‡
        memory_vars = self.memory.load_memory_variables({})
        msg_count = len(memory_vars.get("chat_history", []))
        print(f"ğŸ’¾ å·²ä¿å­˜åˆ°è®°å¿†ï¼Œå½“å‰å…± {msg_count} æ¡æ¶ˆæ¯")
        
        return answer

    def get_chat_history(self):
        """è·å–å¯¹è¯å†å²"""
        memory_vars = self.memory.load_memory_variables({})
        return memory_vars.get("chat_history", [])
    
    def clear_memory(self):
        """æ¸…ç©ºå¯¹è¯è®°å¿†"""
        self.memory.clear()
        print("ğŸ—‘ï¸ å¯¹è¯è®°å¿†å·²æ¸…ç©º")
    
    def get_last_retrieval_info(self):
        """è·å–æœ€åä¸€æ¬¡æ£€ç´¢çš„è¯¦ç»†ä¿¡æ¯"""
        return {
            "used_knowledge_base": self.used_knowledge_base,
            "used_direct_retrieval": self.used_direct_retrieval,
            "retrieved_docs_count": len(self.last_retrieved_docs),
            "sources": [
                {
                    "source": doc.metadata.get("source", "æœªçŸ¥"),
                    "page": doc.metadata.get("page", "æœªçŸ¥"),
                    "preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                }
                for doc in self.last_retrieved_docs
            ]
        }

    def get_last_call_info(self):
        """è·å–æœ€åä¸€æ¬¡è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯"""
        return self.last_call_info

    def get_last_call_info(self):
        """è·å–æœ€åä¸€æ¬¡è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯"""
        return self.last_call_info
