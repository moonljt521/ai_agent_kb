import os
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from app.core.rag import RAGManager
from app.core.keyword_matcher import KeywordMatcher
from dotenv import load_dotenv

load_dotenv()

class AgentManager:
    def __init__(self):
        # èŽ·å–æ¨¡åž‹æä¾›å•†é…ç½®
        provider = os.getenv("MODEL_PROVIDER", "aliyun").lower()
        self.provider = provider
        
        # æ ¹æ®æä¾›å•†åˆå§‹åŒ– LLM
        if provider == "groq":
            self.llm = ChatOpenAI(
                model=os.getenv("GROQ_LLM_MODEL", "llama-3.3-70b-versatile"),
                openai_api_base="https://api.groq.com/openai/v1",
                openai_api_key=os.getenv("GROQ_API_KEY")
            )
            print(f"âœ… ä½¿ç”¨ Groq æ¨¡åž‹: {os.getenv('GROQ_LLM_MODEL', 'llama-3.3-70b-versatile')}")
            print("â„¹ï¸  Groq ä½¿ç”¨ç®€åŒ–çš„ RAG æ¨¡å¼ï¼ˆä¸ä½¿ç”¨ Agentï¼‰")
        else:  # é»˜è®¤ä½¿ç”¨é˜¿é‡Œäº‘
            self.llm = ChatOpenAI(
                model=os.getenv("LLM_MODEL", "qwen-plus"),
                openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
                openai_api_key=os.getenv("DASHSCOPE_API_KEY")
            )
            print(f"âœ… ä½¿ç”¨é˜¿é‡Œäº‘æ¨¡åž‹: {os.getenv('LLM_MODEL', 'qwen-plus')}")
        
        self.rag = RAGManager()
        self.keyword_matcher = KeywordMatcher()  # æ–°å¢žï¼šå…³é”®è¯åŒ¹é…å™¨
        self.last_retrieved_docs = []  # è®°å½•æœ€åŽæ£€ç´¢çš„æ–‡æ¡£
        self.used_knowledge_base = False  # æ ‡è®°æ˜¯å¦ä½¿ç”¨äº†çŸ¥è¯†åº“
        self.used_direct_retrieval = False  # æ ‡è®°æ˜¯å¦ä½¿ç”¨äº†ç›´æŽ¥æ£€ç´¢ï¼ˆä¸èµ°LLMï¼‰
        
        # æ‰“å°å…³é”®è¯ç»Ÿè®¡
        stats = self.keyword_matcher.get_statistics()
        print(f"ðŸ“š å·²åŠ è½½ {stats['æ€»å…³é”®è¯æ•°']} ä¸ªå…³é”®è¯")
        print("ðŸ’¡ å‘½ä¸­å…³é”®è¯å°†ç›´æŽ¥æ£€ç´¢ï¼ŒèŠ‚çœ LLM è°ƒç”¨")

    def create_agent(self):
        # 1. åˆ›å»ºæ£€ç´¢å™¨
        retriever = self.rag.get_retriever()
        
        # 2. å®šä¹‰å·¥å…·å‡½æ•°
        @tool
        def search_knowledge_base(query: str) -> str:
            """æœç´¢æœ¬åœ°çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯ã€‚å¯¹äºŽä»»ä½•é—®é¢˜ï¼Œéƒ½åº”è¯¥å…ˆä½¿ç”¨æ­¤å·¥å…·æœç´¢çŸ¥è¯†åº“ï¼Œçœ‹æ˜¯å¦æœ‰ç›¸å…³å†…å®¹ã€‚çŸ¥è¯†åº“ä¸­å¯èƒ½åŒ…å«ä¹¦ç±ã€æ–‡æ¡£ã€æŠ€æœ¯èµ„æ–™ç­‰å„ç§å†…å®¹ã€‚"""
            docs = retriever.invoke(query)
            # è®°å½•æ£€ç´¢åˆ°çš„æ–‡æ¡£
            self.last_retrieved_docs = docs
            self.used_knowledge_base = True
            return "\n\n".join([d.page_content for d in docs])

        tools = [search_knowledge_base]

        # 3. åˆ›å»º Agent (æ–°ç‰ˆæœ¬ LangChain è¿”å›žçš„æ˜¯ä¸€ä¸ªç¼–è¯‘åŽçš„å›¾)
        return create_agent(
            model=self.llm,
            tools=tools,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚å¯¹äºŽç”¨æˆ·çš„ä»»ä½•é—®é¢˜ï¼Œä½ éƒ½åº”è¯¥å…ˆä½¿ç”¨ search_knowledge_base å·¥å…·æœç´¢æœ¬åœ°çŸ¥è¯†åº“ã€‚å¦‚æžœçŸ¥è¯†åº“ä¸­æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·åŸºäºŽçŸ¥è¯†åº“å†…å®¹å›žç­”ï¼›å¦‚æžœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œå†ä½¿ç”¨ä½ çš„é€šç”¨çŸ¥è¯†å›žç­”ã€‚"
        )

    def run_simple_rag(self, query: str):
        """ç®€åŒ–çš„ RAG å®žçŽ°ï¼Œä¸ä½¿ç”¨ Agentï¼ˆé€‚ç”¨äºŽ Groqï¼‰"""
        # é‡ç½®çŠ¶æ€
        self.last_retrieved_docs = []
        self.used_knowledge_base = False
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retriever = self.rag.get_retriever()
        docs = retriever.invoke(query)
        self.last_retrieved_docs = docs
        
        # 2. æž„å»ºæç¤ºè¯
        if docs:
            self.used_knowledge_base = True
            context = "\n\n".join([doc.page_content for doc in docs])
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºŽä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

çŸ¥è¯†åº“å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºŽä¸Šè¿°çŸ¥è¯†åº“å†…å®¹å›žç­”é—®é¢˜ã€‚å¦‚æžœçŸ¥è¯†åº“å†…å®¹ä¸è¶³ä»¥å›žç­”é—®é¢˜ï¼Œå¯ä»¥ç»“åˆä½ çš„é€šç”¨çŸ¥è¯†è¡¥å……ã€‚"""
        else:
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
        
        # 3. è°ƒç”¨ LLM
        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        
        return response.content

    def direct_retrieval(self, query: str) -> str:
        """
        ç›´æŽ¥æ£€ç´¢æ¨¡å¼ - ä¸ä½¿ç”¨ LLMï¼Œç›´æŽ¥è¿”å›žå‘é‡åº“æ£€ç´¢ç»“æžœ
        é€‚ç”¨äºŽå‘½ä¸­å…³é”®è¯çš„ç®€å•æŸ¥è¯¢
        """
        # é‡ç½®çŠ¶æ€
        self.last_retrieved_docs = []
        self.used_knowledge_base = True
        self.used_direct_retrieval = True
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retriever = self.rag.get_retriever()
        docs = retriever.invoke(query)
        self.last_retrieved_docs = docs
        
        if not docs:
            return "æŠ±æ­‰ï¼Œåœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"
        
        # ç›´æŽ¥è¿”å›žæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼ˆä¸ç»è¿‡ LLM åŠ å·¥ï¼‰
        # å–å‰3ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
        result_parts = []
        for i, doc in enumerate(docs[:3], 1):
            content = doc.page_content.strip()
            source = doc.metadata.get("source", "æœªçŸ¥")
            page = doc.metadata.get("page", "æœªçŸ¥")
            
            result_parts.append(f"ã€ç‰‡æ®µ {i}ã€‘ï¼ˆæ¥æºï¼š{source}ï¼Œé¡µç ï¼š{page}ï¼‰\n{content}")
        
        return "\n\n" + "\n\n".join(result_parts)

    def run(self, query: str):
        # å…ˆæ£€æŸ¥æ˜¯å¦å‘½ä¸­å…³é”®è¯
        should_direct, reason = self.keyword_matcher.should_use_direct_retrieval(query)
        
        if should_direct:
            print(f"ðŸŽ¯ {reason}")
            return self.direct_retrieval(query)
        else:
            print(f"ðŸ¤– {reason}")
            # Groq ä½¿ç”¨ç®€åŒ–çš„ RAGï¼Œé˜¿é‡Œäº‘ä½¿ç”¨ Agent
            if self.provider == "groq":
                return self.run_simple_rag(query)
            else:
                # é‡ç½®çŠ¶æ€
                self.last_retrieved_docs = []
                self.used_knowledge_base = False
                self.used_direct_retrieval = False
                
                graph = self.create_agent()
                # è°ƒç”¨å›¾ï¼Œè¾“å…¥æ¶ˆæ¯åˆ—è¡¨
                inputs = {"messages": [{"role": "user", "content": query}]}
                result = graph.invoke(inputs)
                # èŽ·å–æœ€åŽä¸€æ¡ AI æ¶ˆæ¯çš„å†…å®¹
                messages = result.get("messages", [])
                if messages:
                    return messages[-1].content
                return "æœªèƒ½ç”Ÿæˆå›žå¤ã€‚"
    
    def get_last_retrieval_info(self):
        """èŽ·å–æœ€åŽä¸€æ¬¡æ£€ç´¢çš„è¯¦ç»†ä¿¡æ¯"""
        return {
            "used_knowledge_base": self.used_knowledge_base,
            "used_direct_retrieval": self.used_direct_retrieval,  # æ–°å¢ž
            "retrieved_docs_count": len(self.last_retrieved_docs),
            "sources": [
                {
                    "source": doc.metadata.get("source", "æœªçŸ¥"),
                    "page": doc.metadata.get("page", "æœªçŸ¥"),
                    "preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                }
                for doc in self.last_retrieved_docs
            ]
        }
