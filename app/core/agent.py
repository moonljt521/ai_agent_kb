import os
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from app.core.rag import RAGManager
from app.core.tools import get_all_tools
from dotenv import load_dotenv

load_dotenv()

class SimpleMemory:
    """ç®€å•çš„å¯¹è¯è®°å¿†ï¼Œä¿ç•™æœ€è¿‘ k è½®å¯¹è¯"""
    def __init__(self, k=5):
        self.k = k
        self.messages = []
    
    def save_context(self, inputs, outputs):
        """ä¿å­˜å¯¹è¯"""
        self.messages.append(HumanMessage(content=inputs["input"]))
        self.messages.append(AIMessage(content=outputs["output"]))
        # åªä¿ç•™æœ€è¿‘ k è½®ï¼ˆk*2 æ¡æ¶ˆæ¯ï¼‰
        if len(self.messages) > self.k * 2:
            self.messages = self.messages[-(self.k * 2):]
    
    def load_memory_variables(self, inputs=None):
        """åŠ è½½è®°å¿†"""
        return {"chat_history": self.messages}
    
    def clear(self):
        """æ¸…ç©ºè®°å¿†"""
        self.messages = []


class AgentManager:
    def __init__(self, session_id: str = "default"):
        # è·å–æ¨¡å‹æä¾›å•†é…ç½®
        provider = os.getenv("MODEL_PROVIDER", "aliyun").lower()
        self.provider = provider
        self.session_id = session_id
        
        # æ ¹æ®æä¾›å•†åˆå§‹åŒ– LLM
        if provider == "groq":
            self.llm = ChatOpenAI(
                model=os.getenv("GROQ_LLM_MODEL", "llama-3.3-70b-versatile"),
                openai_api_base="https://api.groq.com/openai/v1",
                openai_api_key=os.getenv("GROQ_API_KEY")
            )
            print(f"âœ… ä½¿ç”¨ Groq æ¨¡å‹: {os.getenv('GROQ_LLM_MODEL', 'llama-3.3-70b-versatile')}")
            print("â„¹ï¸  Groq ä½¿ç”¨ç®€åŒ–çš„ RAG æ¨¡å¼ï¼ˆä¸ä½¿ç”¨ Agentï¼‰")
        else:  # é»˜è®¤ä½¿ç”¨é˜¿é‡Œäº‘
            self.llm = ChatOpenAI(
                model=os.getenv("LLM_MODEL", "qwen-plus"),
                openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
                openai_api_key=os.getenv("DASHSCOPE_API_KEY")
            )
            print(f"âœ… ä½¿ç”¨é˜¿é‡Œäº‘æ¨¡å‹: {os.getenv('LLM_MODEL', 'qwen-plus')}")
        
        self.rag = RAGManager()
        
        # æ–°å¢ï¼šè®°å¿†ç³»ç»Ÿï¼ˆä¿ç•™æœ€è¿‘5è½®å¯¹è¯ï¼‰
        self.memory = SimpleMemory(k=5)
        
        # è®°å½•çŠ¶æ€
        self.last_call_info = {
            "mode": None,
            "llm_called": False,
            "tools_used": [],
            "keyword_matched": None,
        }
        self.last_retrieved_docs = []
        self.used_knowledge_base = False
        self.used_direct_retrieval = False
        
        # æ‰“å°å…³é”®è¯ç»Ÿè®¡
        print(f"ğŸ§  è®°å¿†ç³»ç»Ÿå·²å¯ç”¨ï¼ˆä¿ç•™æœ€è¿‘ 5 è½®å¯¹è¯ï¼‰")

    def create_agent(self, chat_history=None):
        # 1. åˆ›å»ºæ£€ç´¢å™¨
        retriever = self.rag.get_retriever()
        
        # 2. å®šä¹‰çŸ¥è¯†åº“æ£€ç´¢å·¥å…·
        @tool
        def search_knowledge_base(query: str) -> str:
            """æœç´¢æœ¬åœ°çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯ã€‚å¯¹äºä»»ä½•é—®é¢˜ï¼Œéƒ½åº”è¯¥å…ˆä½¿ç”¨æ­¤å·¥å…·æœç´¢çŸ¥è¯†åº“ï¼Œçœ‹æ˜¯å¦æœ‰ç›¸å…³å†…å®¹ã€‚çŸ¥è¯†åº“ä¸­åŒ…å«å››å¤§åè‘—ï¼ˆçº¢æ¥¼æ¢¦ã€ä¸‰å›½æ¼”ä¹‰ã€è¥¿æ¸¸è®°ã€æ°´æµ’ä¼ ï¼‰çš„å®Œæ•´å†…å®¹ã€‚"""
            docs = retriever.invoke(query)
            # è®°å½•æ£€ç´¢åˆ°çš„æ–‡æ¡£
            self.last_retrieved_docs = docs
            self.used_knowledge_base = True
            
            # é˜²å¹»è§‰æœºåˆ¶ï¼šå¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£ï¼Œæ˜ç¡®è¿”å›
            if not docs:
                return "ã€çŸ¥è¯†åº“æ£€ç´¢ç»“æœã€‘æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚è¯·å‘ŠçŸ¥ç”¨æˆ·ï¼šçŸ¥è¯†åº“ä¸­æ²¡æœ‰å…³äºè¿™ä¸ªé—®é¢˜çš„ä¿¡æ¯ã€‚"
            
            # é˜²å¹»è§‰æœºåˆ¶ï¼šå¦‚æœæ£€ç´¢ç»“æœå¤ªå°‘ï¼Œæ ‡æ³¨è­¦å‘Š
            if len(docs) < 2:
                return f"ã€çŸ¥è¯†åº“æ£€ç´¢ç»“æœã€‘ä»…æ‰¾åˆ°å°‘é‡ç›¸å…³å†…å®¹ï¼Œè¯·è°¨æ…å›ç­”ï¼š\n\n{docs[0].page_content}\n\nã€æ³¨æ„ã€‘å¦‚æœå†…å®¹ä¸è¶³ä»¥å®Œæ•´å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ã€‚"
            
            return "\n\n".join([d.page_content for d in docs])

        # 3. è·å–æ‰€æœ‰å·¥å…·ï¼ˆçŸ¥è¯†åº“æ£€ç´¢ + é€šç”¨å·¥å…·ï¼‰
        tools = [search_knowledge_base] + get_all_tools()
        
        print(f"ğŸ”§ Agent å·²åŠ è½½ {len(tools)} ä¸ªå·¥å…·ï¼š")
        for i, t in enumerate(tools, 1):
            print(f"   {i}. {t.name} - {t.description[:50]}...")

        # 4. æ„å»ºç³»ç»Ÿæç¤ºï¼ˆåŒ…å«å¯¹è¯å†å²ï¼‰
        history_text = ""
        if chat_history:
            history_text = "\n\nã€å¯¹è¯å†å²ã€‘\n"
            for msg in chat_history:
                role = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "AI"
                history_text += f"{role}: {msg.content}\n"
            history_text += "\næ³¨æ„ï¼šç†è§£å¯¹è¯å†å²ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œç‰¹åˆ«æ˜¯ä»£è¯ï¼ˆå¦‚\"ä»–\"ã€\"è¿™æœ¬ä¹¦\"ï¼‰çš„æŒ‡ä»£å…³ç³»ã€‚\n"

        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ AI Agentï¼Œä¸“é—¨å›ç­”å…³äºä¸­å›½å››å¤§åè‘—çš„é—®é¢˜ã€‚
{history_text}
ã€ä½ çš„èƒ½åŠ›ã€‘
1. æœç´¢çŸ¥è¯†åº“ï¼šsearch_knowledge_base - æœç´¢å››å¤§åè‘—çš„å®Œæ•´å†…å®¹
2. æ•°å­¦è®¡ç®—ï¼šcalculator - è¿›è¡Œæ•°å­¦è¿ç®—
3. æ—¶é—´æŸ¥è¯¢ï¼šget_current_time - è·å–å½“å‰æ—¶é—´
4. æ–‡æœ¬ç»Ÿè®¡ï¼šcount_characters - ç»Ÿè®¡æ–‡æœ¬ä¿¡æ¯
5. æ–‡æœ¬æœç´¢ï¼štext_search - åœ¨æ–‡æœ¬ä¸­æœç´¢å…³é”®è¯
6. æ•°å­—æ¯”è¾ƒï¼šcompare_numbers - æ¯”è¾ƒæ•°å­—å¤§å°
7. åè‘—åˆ—è¡¨ï¼šlist_four_classics - åˆ—å‡ºå››å¤§åè‘—ä¿¡æ¯
8. ä¹¦ç±ä¿¡æ¯ï¼šget_book_info - è·å–æŒ‡å®šä¹¦ç±è¯¦æƒ…

ã€å·¥ä½œæµç¨‹ã€‘
1. åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­éœ€è¦ä½¿ç”¨å“ªäº›å·¥å…·
2. å¦‚æœé—®é¢˜æ¶‰åŠå››å¤§åè‘—å†…å®¹ï¼Œå…ˆä½¿ç”¨ search_knowledge_base æœç´¢
3. å¦‚æœéœ€è¦è®¡ç®—ã€æ—¶é—´ç­‰ä¿¡æ¯ï¼Œä½¿ç”¨ç›¸åº”çš„å·¥å…·
4. å¯ä»¥ç»„åˆä½¿ç”¨å¤šä¸ªå·¥å…·æ¥å®Œæˆå¤æ‚ä»»åŠ¡
5. åŸºäºå·¥å…·è¿”å›çš„ç»“æœï¼Œç»™å‡ºå®Œæ•´å‡†ç¡®çš„ç­”æ¡ˆ
6. æ³¨æ„å¯¹è¯å†å²ï¼Œç†è§£ä»£è¯æŒ‡ä»£

ã€æ ¸å¿ƒè§„åˆ™ã€‘
1. å¯¹äºå››å¤§åè‘—çš„é—®é¢˜ï¼Œå¿…é¡»å…ˆä½¿ç”¨ search_knowledge_base æœç´¢
2. åªèƒ½åŸºäºå·¥å…·è¿”å›çš„ç»“æœå›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
3. å¦‚æœå·¥å…·è¿”å›"æœªæ‰¾åˆ°"ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
4. å¯ä»¥ä½¿ç”¨å¤šä¸ªå·¥å…·æ¥å®Œæˆä»»åŠ¡
5. ä¿æŒå›ç­”çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
6. ç»“åˆå¯¹è¯å†å²ç†è§£é—®é¢˜ä¸­çš„ä»£è¯æŒ‡ä»£

ã€ç¤ºä¾‹ã€‘
ç”¨æˆ·ï¼š"çº¢æ¥¼æ¢¦æœ‰å¤šå°‘å›ï¼Ÿ"
æ€è€ƒï¼šè¿™æ˜¯å…³äºä¹¦ç±ä¿¡æ¯çš„é—®é¢˜
è¡ŒåŠ¨ï¼šä½¿ç”¨ get_book_info å·¥å…·
ç»“æœï¼šã€Šçº¢æ¥¼æ¢¦ã€‹æœ‰120å›

ç”¨æˆ·ï¼š"è®¡ç®—ä¸€ä¸‹ 123 + 456"
æ€è€ƒï¼šè¿™æ˜¯æ•°å­¦è®¡ç®—é—®é¢˜
è¡ŒåŠ¨ï¼šä½¿ç”¨ calculator å·¥å…·
ç»“æœï¼š579

ç”¨æˆ·ï¼š"è´¾å®ç‰å’Œæ—é»›ç‰çš„å…³ç³»"
æ€è€ƒï¼šè¿™éœ€è¦æŸ¥è¯¢çŸ¥è¯†åº“
è¡ŒåŠ¨ï¼šä½¿ç”¨ search_knowledge_base å·¥å…·
ç»“æœï¼šæ ¹æ®çŸ¥è¯†åº“å†…å®¹å›ç­”..."""

        # 5. åˆ›å»º Agent (æ–°ç‰ˆæœ¬ LangChain è¿”å›çš„æ˜¯ä¸€ä¸ªç¼–è¯‘åçš„å›¾)
        return create_agent(
            model=self.llm,
            tools=tools,
            system_prompt=system_prompt
        )

    def run_simple_rag(self, query: str):
        """ç®€åŒ–çš„ RAG å®ç°ï¼Œä¸ä½¿ç”¨ Agentï¼ˆé€‚ç”¨äº Groqï¼‰
        åŒ…å«é˜²å¹»è§‰æœºåˆ¶å’Œå¯¹è¯å†å²"""
        # é‡ç½®çŠ¶æ€
        self.last_retrieved_docs = []
        self.used_knowledge_base = False
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retriever = self.rag.get_retriever()
        docs = retriever.invoke(query)
        self.last_retrieved_docs = docs
        
        # 2. è·å–å¯¹è¯å†å²
        memory_vars = self.memory.load_memory_variables({})
        chat_history = memory_vars.get("chat_history", [])
        history_text = ""
        if chat_history:
            history_text = "\nã€å¯¹è¯å†å²ã€‘\n"
            for msg in chat_history:
                role = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "AI"
                history_text += f"{role}: {msg.content}\n"
            history_text += "\n"
        
        # 3. æ„å»ºæç¤ºè¯ï¼ˆé˜²å¹»è§‰ä¼˜åŒ–ï¼‰
        if not docs:
            # æ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚

{history_text}ã€é‡è¦æç¤ºã€‘
çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”ã€‘
æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚è¯·å°è¯•æ¢ä¸€ä¸ªé—®é¢˜æˆ–æä¾›æ›´å¤šä¸Šä¸‹æ–‡ã€‚"""
        elif len(docs) < 2:
            # æ£€ç´¢ç»“æœå¤ªå°‘
            self.used_knowledge_base = True
            context = docs[0].page_content
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚

{history_text}ã€é‡è¦æç¤ºã€‘
çŸ¥è¯†åº“ä¸­åªæ‰¾åˆ°äº†å°‘é‡ç›¸å…³å†…å®¹ï¼Œè¯·è°¨æ…å›ç­”ã€‚

ã€çŸ¥è¯†åº“å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- åªåŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”
- å¦‚æœå†…å®¹ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
- ä¸è¦ç¼–é€ æˆ–æ¨æµ‹
- æ³¨æ„å¯¹è¯å†å²ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œç†è§£ä»£è¯æŒ‡ä»£"""
        else:
            # æ­£å¸¸æƒ…å†µ
            self.used_knowledge_base = True
            context = "\n\n".join([doc.page_content for doc in docs])
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

{history_text}ã€æ ¸å¿ƒè§„åˆ™ã€‘
1. åªèƒ½åŸºäºä¸‹æ–¹æä¾›çš„çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜
2. å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œæ˜ç¡®è¯´æ˜
3. ä¸è¦ç¼–é€ ã€æ¨æµ‹æˆ–ä½¿ç”¨çŸ¥è¯†åº“å¤–çš„ä¿¡æ¯
4. å¼•ç”¨åŸæ–‡æ—¶è¦å‡†ç¡®ï¼Œä¸è¦ç¯¡æ”¹æˆ–è¿‡åº¦è§£è¯»
5. æ³¨æ„å¯¹è¯å†å²ï¼Œç†è§£ä»£è¯ï¼ˆå¦‚\"ä»–\"ã€\"è¿™æœ¬ä¹¦\"ï¼‰çš„æŒ‡ä»£å…³ç³»

ã€çŸ¥è¯†åº“å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- å¦‚æœçŸ¥è¯†åº“ä¸­æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œè¯·è¯¦ç»†å›ç­”
- å¦‚æœçŸ¥è¯†åº“ä¸­åªæœ‰éƒ¨åˆ†ä¿¡æ¯ï¼Œè¯´æ˜"æ ¹æ®çŸ¥è¯†åº“å†…å®¹ï¼Œå¯ä»¥å›ç­”ä»¥ä¸‹éƒ¨åˆ†ï¼š..."
- å¦‚æœçŸ¥è¯†åº“ä¸­å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥å›ç­”"æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰å…³äºè¿™ä¸ªé—®é¢˜çš„ä¿¡æ¯"
- å›ç­”æ—¶å¯ä»¥é€‚å½“å¼•ç”¨åŸæ–‡ç‰‡æ®µï¼Œç”¨å¼•å·æ ‡æ³¨
- ç»“åˆå¯¹è¯å†å²ç†è§£é—®é¢˜ä¸­çš„ä»£è¯æŒ‡ä»£"""
        
        # 4. è°ƒç”¨ LLM
        
        # æ‰“å°å‘é€ç»™ LLM çš„ prompt
        print("\n" + "="*80)
        print("ğŸ“¤ å‘é€ç»™ LLM çš„ Prompt (Simple RAG)ï¼š")
        print("="*80)
        print(prompt)
        print("="*80 + "\n")
        
        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        
        return response.content

    def direct_retrieval(self, query: str) -> str:
        """
        ç›´æ¥æ£€ç´¢æ¨¡å¼ - å‘½ä¸­å…³é”®è¯åï¼Œæ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶é€šè¿‡ LLM ç”Ÿæˆç­”æ¡ˆ
        è¿™æ˜¯ä¼˜åŒ–åçš„ RAG æµç¨‹ï¼šè·³è¿‡ Agent å·¥å…·è°ƒç”¨ï¼Œç›´æ¥æ£€ç´¢ + LLM ç”Ÿæˆ
        åŒ…å«é˜²å¹»è§‰æœºåˆ¶ï¼šç›¸ä¼¼åº¦é˜ˆå€¼ã€çŸ¥è¯†åº“è¦†ç›–ç‡æ£€æŸ¥ã€å¯¹è¯å†å²
        """
        # é‡ç½®çŠ¶æ€
        self.last_retrieved_docs = []
        self.used_knowledge_base = True
        self.used_direct_retrieval = True
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retriever = self.rag.get_retriever()
        docs = retriever.invoke(query)
        self.last_retrieved_docs = docs
        
        # 2. è·å–å¯¹è¯å†å²
        memory_vars = self.memory.load_memory_variables({})
        chat_history = memory_vars.get("chat_history", [])
        history_text = ""
        if chat_history:
            history_text = "\nã€å¯¹è¯å†å²ã€‘\n"
            for msg in chat_history:
                role = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "AI"
                history_text += f"{role}: {msg.content}\n"
            history_text += "\n"
        
        # 3. çŸ¥è¯†åº“è¦†ç›–ç‡æ£€æŸ¥ï¼ˆé˜²å¹»è§‰æœºåˆ¶ 1ï¼‰
        if not docs:
            # æ²¡æœ‰æ£€ç´¢åˆ°ä»»ä½•æ–‡æ¡£
            return "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚è¯·å°è¯•æ¢ä¸€ä¸ªé—®é¢˜æˆ–æä¾›æ›´å¤šä¸Šä¸‹æ–‡ã€‚"
        
        # 4. ç›¸ä¼¼åº¦é˜ˆå€¼æ£€æŸ¥ï¼ˆé˜²å¹»è§‰æœºåˆ¶ 2ï¼‰
        if len(docs) < 2:
            # æ£€ç´¢ç»“æœå¤ªå°‘ï¼Œå¯èƒ½ç›¸å…³æ€§ä¸é«˜
            context = docs[0].page_content
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚

{history_text}ã€é‡è¦æç¤ºã€‘
çŸ¥è¯†åº“ä¸­åªæ‰¾åˆ°äº†å°‘é‡ç›¸å…³å†…å®¹ï¼Œè¯·è°¨æ…å›ç­”ã€‚å¦‚æœå†…å®¹ä¸è¶³ä»¥å®Œæ•´å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

ã€çŸ¥è¯†åº“å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- å¦‚æœå†…å®¹è¶³å¤Ÿï¼Œè¯·åŸºäºçŸ¥è¯†åº“å›ç­”
- å¦‚æœå†…å®¹ä¸è¶³ï¼Œè¯·è¯´æ˜"çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯æœ‰é™ï¼Œåªèƒ½æä¾›ä»¥ä¸‹å†…å®¹ï¼š..."
- ä¸è¦ç¼–é€ æˆ–æ¨æµ‹çŸ¥è¯†åº“å¤–çš„ä¿¡æ¯
- æ³¨æ„å¯¹è¯å†å²ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œç†è§£ä»£è¯æŒ‡ä»£"""
            
            messages = [HumanMessage(content=prompt)]
            response = self.llm.invoke(messages)
            return response.content
        
        # 5. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆæ­£å¸¸æƒ…å†µï¼‰
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # 6. æ„å»ºå¢å¼ºæç¤ºè¯ï¼ˆRAG æ ¸å¿ƒ - é˜²æ­¢å¹»è§‰ï¼‰
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

{history_text}ã€æ ¸å¿ƒè§„åˆ™ã€‘
1. åªèƒ½åŸºäºä¸‹æ–¹æä¾›çš„çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜
2. å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œæ˜ç¡®è¯´æ˜"çŸ¥è¯†åº“ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜"
3. ä¸è¦ç¼–é€ ã€æ¨æµ‹æˆ–ä½¿ç”¨çŸ¥è¯†åº“å¤–çš„ä¿¡æ¯
4. å¼•ç”¨åŸæ–‡æ—¶è¦å‡†ç¡®ï¼Œä¸è¦ç¯¡æ”¹æˆ–è¿‡åº¦è§£è¯»
5. ä¿æŒå®¢è§‚ä¸­ç«‹ï¼Œä¸è¦æ·»åŠ ä¸ªäººè§‚ç‚¹
6. æ³¨æ„å¯¹è¯å†å²ï¼Œç†è§£ä»£è¯ï¼ˆå¦‚\"ä»–\"ã€\"è¿™æœ¬ä¹¦\"ï¼‰çš„æŒ‡ä»£å…³ç³»

ã€çŸ¥è¯†åº“å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- å¦‚æœçŸ¥è¯†åº“ä¸­æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œè¯·è¯¦ç»†å›ç­”
- å¦‚æœçŸ¥è¯†åº“ä¸­åªæœ‰éƒ¨åˆ†ä¿¡æ¯ï¼Œè¯´æ˜"æ ¹æ®çŸ¥è¯†åº“å†…å®¹ï¼Œå¯ä»¥å›ç­”ä»¥ä¸‹éƒ¨åˆ†ï¼š..."
- å¦‚æœçŸ¥è¯†åº“ä¸­å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥å›ç­”"æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰å…³äºè¿™ä¸ªé—®é¢˜çš„ä¿¡æ¯"
- å›ç­”æ—¶å¯ä»¥é€‚å½“å¼•ç”¨åŸæ–‡ç‰‡æ®µï¼Œç”¨å¼•å·æ ‡æ³¨
- ç»“åˆå¯¹è¯å†å²ç†è§£é—®é¢˜ä¸­çš„ä»£è¯æŒ‡ä»£"""
        
        # 7. è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        
        return response.content

    def run(self, query: str):
        # é‡ç½®è°ƒç”¨ä¿¡æ¯
        self.last_call_info = {
            "mode": None,
            "llm_called": False,
            "tools_used": [],
            "keyword_matched": None,
        }
        
        # Groq ä½¿ç”¨ç®€åŒ–çš„ RAGï¼Œé˜¿é‡Œäº‘ä½¿ç”¨ Agent
        if self.provider == "groq":
            print("ğŸ¤– ä½¿ç”¨ Groq ç®€åŒ– RAG æ¨¡å¼")
            self.last_call_info["mode"] = "simple_rag"
            self.last_call_info["llm_called"] = True
            answer = self.run_simple_rag(query)
        else:
            print("ğŸ¤– ä½¿ç”¨ Agent æ¨ç†æ¨¡å¼")
            self.last_call_info["mode"] = "agent"
            self.last_call_info["llm_called"] = True
            
            # é‡ç½®çŠ¶æ€
            self.last_retrieved_docs = []
            self.used_knowledge_base = False
            
            # è·å–å¯¹è¯å†å²
            memory_vars = self.memory.load_memory_variables({})
            chat_history = memory_vars.get("chat_history", [])
            
            # åˆ›å»º Agentï¼ˆä¼ å…¥å¯¹è¯å†å²ï¼‰
            graph = self.create_agent(chat_history=chat_history)
            
            # è°ƒç”¨å›¾ï¼Œè¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            
            inputs = {"messages": [{"role": "user", "content": query}]}
            
            # æ‰“å°å‘é€ç»™ LLM çš„ prompt
            print("\n" + "="*80)
            print("ğŸ“¤ å‘é€ç»™ LLM çš„æ¶ˆæ¯ï¼š")
            print("="*80)
            for msg in inputs["messages"]:
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                print(f"\n[{role.upper()}]")
                print(content)
            if chat_history:
                print(f"\n[å¯¹è¯å†å²] {len(chat_history)} æ¡æ¶ˆæ¯")
                for msg in chat_history[-4:]:
                    role = "ç”¨æˆ·" if msg.__class__.__name__ == "HumanMessage" else "AI"
                    content_preview = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
                    print(f"  - {role}: {content_preview}")
            print("="*80 + "\n")
            
            result = graph.invoke(inputs)
            # è·å–æœ€åä¸€æ¡ AI æ¶ˆæ¯çš„å†…å®¹
            messages = result.get("messages", [])
            if messages:
                answer = messages[-1].content
            else:
                answer = "æœªèƒ½ç”Ÿæˆå›å¤ã€‚"
        
        # ä¿å­˜åˆ°è®°å¿†
        self.memory.save_context(
            {"input": query},
            {"output": answer}
        )
        
        # è·å–å½“å‰è®°å¿†ä¸­çš„æ¶ˆæ¯æ•°é‡
        memory_vars = self.memory.load_memory_variables({})
        msg_count = len(memory_vars.get("chat_history", []))
        print(f"ğŸ’¾ å·²ä¿å­˜åˆ°è®°å¿†ï¼Œå½“å‰å…± {msg_count} æ¡æ¶ˆæ¯")
        
        return answer

    def get_chat_history(self):
        """è·å–å¯¹è¯å†å²"""
        memory_vars = self.memory.load_memory_variables({})
        return memory_vars.get("chat_history", [])
    
    def clear_memory(self):
        """æ¸…ç©ºå¯¹è¯è®°å¿†"""
        self.memory.clear()
        print("ğŸ—‘ï¸ å¯¹è¯è®°å¿†å·²æ¸…ç©º")
    
    def get_last_retrieval_info(self):
        """è·å–æœ€åä¸€æ¬¡æ£€ç´¢çš„è¯¦ç»†ä¿¡æ¯"""
        return {
            "used_knowledge_base": self.used_knowledge_base,
            "used_direct_retrieval": self.used_direct_retrieval,
            "retrieved_docs_count": len(self.last_retrieved_docs),
            "sources": [
                {
                    "source": doc.metadata.get("source", "æœªçŸ¥"),
                    "page": doc.metadata.get("page", "æœªçŸ¥"),
                    "preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                }
                for doc in self.last_retrieved_docs
            ]
        }

    def get_last_call_info(self):
        """è·å–æœ€åä¸€æ¬¡è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯"""
        return self.last_call_info

    def get_last_call_info(self):
        """è·å–æœ€åä¸€æ¬¡è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯"""
        return self.last_call_info
