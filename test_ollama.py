#!/usr/bin/env python3
"""æµ‹è¯• Ollama é…ç½®"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

load_dotenv()

def test_ollama_connection():
    """æµ‹è¯• Ollama è¿æ¥"""
    print("========================================")
    print("ğŸ§ª æµ‹è¯• Ollama é…ç½®")
    print("========================================\n")
    
    # è¯»å–é…ç½®
    provider = os.getenv("MODEL_PROVIDER", "aliyun")
    ollama_url = os.getenv("OLLAMA_BASE_URL", "http://127.0.0.1:11434")
    ollama_model = os.getenv("OLLAMA_LLM_MODEL", "qwen3:8b")
    
    print(f"æä¾›å•†: {provider}")
    print(f"Ollama åœ°å€: {ollama_url}")
    print(f"Ollama æ¨¡å‹: {ollama_model}\n")
    
    if provider != "ollama":
        print(f"âš ï¸  å½“å‰æä¾›å•†æ˜¯ {provider}ï¼Œä¸æ˜¯ ollama")
        print("è¯·è¿è¡Œ: ./switch_provider.sh åˆ‡æ¢åˆ° Ollama\n")
        return False
    
    try:
        # åˆå§‹åŒ– LLM
        print("ğŸ”— è¿æ¥ Ollama...")
        llm = ChatOpenAI(
            model=ollama_model,
            openai_api_base=f"{ollama_url}/v1",
            openai_api_key="ollama"
        )
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        print("ğŸ’¬ å‘é€æµ‹è¯•æ¶ˆæ¯...\n")
        test_query = "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"
        print(f"é—®é¢˜: {test_query}")
        print("å›ç­”: ", end="", flush=True)
        
        messages = [HumanMessage(content=test_query)]
        response = llm.invoke(messages)
        
        print(response.content)
        print("\nâœ… Ollama é…ç½®æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("\nè¯·æ£€æŸ¥:")
        print("1. Ollama æ˜¯å¦æ­£åœ¨è¿è¡Œ: curl http://127.0.0.1:11434/")
        print("2. æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½: ollama list")
        print("3. æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®: qwen3:8b")
        return False

if __name__ == "__main__":
    test_ollama_connection()
